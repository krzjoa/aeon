[{"path":"/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2022 aion authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"/articles/m5_forecasting.html","id":"data-preprocessing","dir":"Articles","previous_headings":"","what":"Data preprocessing","title":"M5 Forecasting competitions","text":"","code":"train <- tiny_m5[date < '2016-01-01'] test  <- tiny_m5[date >= '2016-01-01']  m5_recipe <-   recipe(value ~ ., data=train) %>%   step_mutate(item_id_idx=item_id, store_id_idx=store_id) %>%   step_integer(item_id_idx, store_id_idx,                wday, month,                event_name_1, event_type_1,                event_name_2, event_type_2,                zero_based=TRUE) %>%   step_naomit(all_predictors()) %>%   prep()  train <- bake(m5_recipe, train) test  <- bake(m5_recipe, test)  setDT(train) setDT(test)"},{"path":"/articles/m5_forecasting.html","id":"experiment-config","dir":"Articles","previous_headings":"","what":"Experiment config","title":"M5 Forecasting competitions","text":"","code":"TARGET      <- 'value' STATIC_CAT  <- c('item_id_idx', 'store_id_idx') DYNAMIC_CAT <- c('event_name_1', 'event_type_1') CATEGORICAL <- c(DYNAMIC_CAT, STATIC_CAT) NUMERIC     <- c('sell_price', 'sell_price') KEY         <- c('item_id', 'store_id') INDEX       <- 'date' LOOKBACK    <- 28 HORIZON     <- 14 STRIDE      <- LOOKBACK BATCH_SIZE  <- 32"},{"path":"/articles/m5_forecasting.html","id":"creating-generators","dir":"Articles","previous_headings":"","what":"Creating generators","title":"M5 Forecasting competitions","text":"","code":"c(train_generator, train_steps) %<-%     ts_generator(         data        = train,         key         = KEY,         index       = INDEX,         lookback    = LOOKBACK,         horizon     = HORIZON,         stride      = STRIDE,         target      = TARGET,         static      = STATIC_CAT,         categorical = CATEGORICAL,         numeric     = NUMERIC,         shuffle     = TRUE,         batch_size  = BATCH_SIZE       )  c(test_generator, test_steps) %<-%     ts_generator(         data        = test,         key         = KEY,         index       = INDEX,         lookback    = LOOKBACK,         horizon     = HORIZON,         stride      = STRIDE,         target      = TARGET,         static      = STATIC_CAT,         categorical = CATEGORICAL,         numeric     = NUMERIC,         shuffle     = FALSE,         batch_size  = BATCH_SIZE       )"},{"path":"/articles/m5_forecasting.html","id":"tft-model","dir":"Articles","previous_headings":"","what":"TFT model","title":"M5 Forecasting competitions","text":"","code":"tft <-   model_tft(     lookback                = LOOKBACK,     horizon                 = HORIZON,     past_numeric_size       = length(NUMERIC) + 1,     past_categorical_size   = length(DYNAMIC_CAT),     future_numeric_size     = length(NUMERIC),     future_categorical_size = length(DYNAMIC_CAT),     vocab_static_size       = dict_size(train, STATIC_CAT),     vocab_dynamic_size      = dict_size(train, DYNAMIC_CAT),     hidden_dim              = 10,     state_size              = 5,     num_heads               = 10,     dropout_rate            = NULL,     output_size             = 1   ) #> Loaded Tensorflow version 2.10.0  tft %>%    compile(optimizer='adam', loss='mse')"},{"path":"/articles/m5_forecasting.html","id":"training","dir":"Articles","previous_headings":"","what":"Training","title":"M5 Forecasting competitions","text":"","code":"# tft %>%  #   fit( #     x = train_generator, #     steps_per_epoch = train_steps #   )"},{"path":"/articles/m5_forecasting.html","id":"forecasting","dir":"Articles","previous_headings":"","what":"Forecasting","title":"M5 Forecasting competitions","text":"","code":"# forecast <-  #   tft %>%  #   predict( #     x = test_generator, #     steps_per_epoch = test_steps #   )"},{"path":"/articles/preparing_input.html","id":"data-analysis","dir":"Articles","previous_headings":"","what":"Data analysis","title":"Preparing input for time series models","text":"example ’ll global_economy dataset tsibbledata package. global_economy n example panel dataset. simply means contains multiple time series, distinguished Country ’d like , optional preprocessing scaling, imputation etc. create set arrays (tensors) generator, serves respective batch-level tensors fly.","code":"head(global_economy) #> # A tibble: 6 × 9 #>   Country     Code   Year         GDP Growth   CPI Imports Exports Population #>   <fct>       <fct> <dbl>       <dbl>  <dbl> <dbl>   <dbl>   <dbl>      <dbl> #> 1 Afghanistan AFG    1960  537777811.     NA    NA    7.02    4.13    8996351 #> 2 Afghanistan AFG    1961  548888896.     NA    NA    8.10    4.45    9166764 #> 3 Afghanistan AFG    1962  546666678.     NA    NA    9.35    4.88    9345868 #> 4 Afghanistan AFG    1963  751111191.     NA    NA   16.9     9.17    9533954 #> 5 Afghanistan AFG    1964  800000044.     NA    NA   18.1     8.89    9731361 #> 6 Afghanistan AFG    1965 1006666638.     NA    NA   21.4    11.3     9938414 summary(global_economy$Year) #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>    1960    1974    1989    1989    2003    2017 glob_econ <- as.data.table(global_economy)  sample_countries <- sample(unique(global_economy$Country), 4)  ggplot(glob_econ[Country %in% sample_countries]) +   geom_line(aes(Year, GDP)) +   facet_wrap(vars(Country), scales = 'free_y') #> Warning: Removed 31 row(s) containing missing values (geom_path)."},{"path":"/articles/preparing_input.html","id":"data-preprocessing","dir":"Articles","previous_headings":"","what":"Data preprocessing","title":"Preparing input for time series models","text":"","code":"split_year <- 2000  ge_recipe <-    recipe(GDP ~ . , data = glob_econ) %>%    step_mutate(Country_idx = Country) %>%    step_integer(Country_idx) %>%    prep()  train <-    glob_econ[Year <= split_year] %>%    bake(ge_recipe, .)  test <-    glob_econ[Year > split_year] %>%    bake(ge_recipe, .)"},{"path":"/articles/preparing_input.html","id":"arrays","dir":"Articles","previous_headings":"","what":"Arrays","title":"Preparing input for time series models","text":"Typically, time series forecasting field, can distiguish following types variables: past values target variable future values target variable past dynamic features (numeric categorical) future dynamic features (numeric categorical) static features (numeric categorical) Past target values can simply treated part tensor past dynamic features. simplest possible scenario split dataset using certain date. can see, contains five types arrays. One , y_fut target, rest may used model input. demonstrate, can manipulate data make_arrays function, let’s add static variables separarte array past GDP values.","code":"KEY         <- 'Country' INDEX       <- 'Year' TARGET      <- 'GDP' NUMERIC     <- c('Growth', 'CPI', 'Imports', 'Exports', 'Population') CATEGORICAL <- 'Country_idx' STATIC      <- 'Country_idx'  LOOKBACK <- 10 HOIRZON  <- 5  train_arrays <-   make_arrays(     data        = train,     key         = KEY,     index       = INDEX,     lookback    = LOOKBACK,     horizon     = HOIRZON,     stride      = 4,     shuffle     = TRUE,     target      = TARGET,     categorical = CATEGORICAL,     numeric     = NUMERIC   ) #> Warning in make_arrays(data = train, key = KEY, index = INDEX, lookback = #> LOOKBACK, : Found samples with end_time - start_time < total_window_length. #> They'll be removed.  print(names(train_arrays)) #> [1] \"X_past_num\" \"X_past_cat\" \"y_fut\"      \"X_fut_num\"  \"X_fut_cat\" print(dim(train_arrays$X_past_num)) #> [1] 1820   10    6 train_arrays <-   make_arrays(     data        = train,     key         = KEY,     index       = INDEX,     lookback    = LOOKBACK,     horizon     = HOIRZON,     stride      = 4,     shuffle     = TRUE,     target      = TARGET,     categorical = CATEGORICAL,     numeric     = NUMERIC,     static      = STATIC,     y_past_sep  = TRUE    ) #> Warning in make_arrays(data = train, key = KEY, index = INDEX, lookback = #> LOOKBACK, : Found samples with end_time - start_time < total_window_length. #> They'll be removed.  print(names(train_arrays)) #> [1] \"y_past\"       \"X_past_num\"   \"X_past_cat\"   \"y_fut\"        \"X_fut_num\"    #> [6] \"X_fut_cat\"    \"X_static_cat\" print(dim(train_arrays$X_past_num)) #> [1] 1820   10    5"},{"path":"/articles/preparing_input.html","id":"generators","dir":"Articles","previous_headings":"","what":"Generators","title":"Preparing input for time series models","text":"can see, output list arrays, list two objects: generator function Returns batch training data every call. loop comes end, counter reset generator starts serving data beginning . number steps argument required keras methods. can pass arguments fit function follows:","code":"c(train_gen, train_n_steps) %<-%   ts_generator(     data        = train,     key         = KEY,     index       = INDEX,     lookback    = LOOKBACK,     horizon     = HOIRZON,     stride      = 4,     shuffle     = TRUE,     target      = TARGET,     categorical = CATEGORICAL,     numeric     = NUMERIC,     static      = STATIC,     y_past_sep  = TRUE,     batch_size  = 32    ) #> Warning in ts_generator(data = train, key = KEY, index = INDEX, lookback = #> LOOKBACK, : Found samples with end_time - start_time < total_window_length. #> They'll be removed.  print(class(train_gen)) #> [1] \"function\" batch <- train_gen() names(batch) #> [1] \"y_past\"       \"X_past_num\"   \"X_past_cat\"   \"y_fut\"        \"X_fut_num\"    #> [6] \"X_fut_cat\"    \"X_static_cat\" print(dim(batch$y_past)) #> [1] 32 10  1 model %>%    fit(     x = train_gen,     steps_per_epoch = train_n_steps,     validation_data = val_gen,     validation_steps = val_n_steps   )  model %>%    predict(     x = test_gen,     steps = test_n_steps   )"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Krzysztof Joachimiak. Author, maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Joachimiak K (2022). aion: Time Series keras. R package version 0.1.0.","code":"@Manual{,   title = {aion: Time Series with keras},   author = {Krzysztof Joachimiak},   year = {2022},   note = {R package version 0.1.0}, }"},{"path":"/index.html","id":"aion","dir":"","previous_headings":"","what":"Time Series with keras","title":"Time Series with keras","text":"Time Series models keras R","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Time Series with keras","text":"can install development version aion GitHub :","code":"# install.packages(\"devtools\") devtools::install_github(\"krzjoa/aion\")"},{"path":"/index.html","id":"key-features","dir":"","previous_headings":"","what":"Key features","title":"Time Series with keras","text":"Temporal Fusion Transformer model additional layers: Temporal Convolutional Network block & Legendre Memory Unit make_array ts_generator functions quickly prepare input/output keras time series models new loss functions: loss_quantile, loss_tweedie loss_negative_log_likelihood","code":""},{"path":"/index.html","id":"usage","dir":"","previous_headings":"","what":"Usage","title":"Time Series with keras","text":"","code":"# Dataset library(m5)  # Neural Networks library(aion) library(keras)  # Data wrangling library(dplyr, warn.conflicts=FALSE) library(data.table, warn.conflicts=FALSE) library(recipes, warn.conflicts=FALSE)  # ========================================================================== #                          PREPARING THE DATA # ==========================================================================  train <- tiny_m5[date < '2016-01-01'] test  <- tiny_m5[date >= '2016-01-01']  m5_recipe <-   recipe(value ~ ., data=train) %>%   step_mutate(item_id_idx=item_id, store_id_idx=store_id) %>%   step_integer(item_id_idx, store_id_idx,                wday, month,                event_name_1, event_type_1,                event_name_2, event_type_2,                zero_based=TRUE) %>%   step_naomit(all_predictors()) %>%   prep()  train <- bake(m5_recipe, train) test  <- bake(m5_recipe, test)  TARGET      <- 'value' STATIC      <- c('item_id_idx', 'store_id_idx') CATEGORICAL <- c('event_name_1', 'event_type_1', STATIC) NUMERIC     <- c('sell_price', 'sell_price') KEY         <- c('item_id', 'store_id') INDEX       <- 'date' LOOKBACK    <- 28 HORIZON     <- 14 STRIDE      <- LOOKBACK BATCH_SIZE  <- 32  # ========================================================================== #                          CREATING GENERATORS # ==========================================================================  c(train_generator, train_steps) %<-%     ts_generator(         data = train,         key = KEY,         index = INDEX,         lookback = LOOKBACK,         horizon = HORIZON,         stride = STRIDE,         target=TARGET,         static=STATIC,         categorical=CATEGORICAL,         numeric=NUMERIC,         batch_size=BATCH_SIZE       )  test_generator <-     ts_generator(         data = test,         key = KEY,         index = INDEX,         lookback = LOOKBACK,         horizon = HORIZON,         stride = STRIDE,         target=TARGET,         static=STATIC,         categorical=CATEGORICAL,         numeric=NUMERIC     )"},{"path":"/reference/aion-package.html","id":null,"dir":"Reference","previous_headings":"","what":"aion: Time Series Models with 'keras' — aion-package","title":"aion: Time Series Models with 'keras' — aion-package","text":"(maybe one line) Use four spaces indenting paragraphs within Description.","code":""},{"path":"/reference/aion-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"aion: Time Series Models with 'keras' — aion-package","text":"Krzysztof Joachimiak","code":""},{"path":"/reference/as_3d_array.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a 3-dimensional array out of a data.frame — as_3d_array","title":"Create a 3-dimensional array out of a data.frame — as_3d_array","text":"key index used create batch_size timesteps dimension respectively. end, excluded data.frame, present anymore final 3-dimensional array. time series lengths must equal.","code":""},{"path":"/reference/as_3d_array.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a 3-dimensional array out of a data.frame — as_3d_array","text":"","code":"as_3d_array(data, index, key)"},{"path":"/reference/as_3d_array.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a 3-dimensional array out of a data.frame — as_3d_array","text":"data data.frame object index time-related column key Columns, creates unique time series IDs","code":""},{"path":"/reference/as_3d_array.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a 3-dimensional array out of a data.frame — as_3d_array","text":"3-dimensional array whith following dimensions: (n_unique_ids, n_timesteps, n_features)","code":""},{"path":"/reference/as_3d_array.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a 3-dimensional array out of a data.frame — as_3d_array","text":"example, subset global_economy dataset {tibbledata} package, 263 countries (key), 58 years (index) two features assigned country/year pair, output array shape (263, 58, 2). Bear mind toy example, split dataset least two parts historical context past expected future target ML model. basic helper function -create complete input tensors, please use make_arrays() function.","code":""},{"path":"/reference/as_3d_array.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a 3-dimensional array out of a data.frame — as_3d_array","text":"","code":"library(tsibbledata) library(dplyr, warn.conflicts=FALSE)  # `global_economy` dataset comes from the `{tsibbledata}` package  selected_ge <-  global_economy %>%  select(Country, Year, Imports, Exports)  tensor <- as_3d_array(selected_ge, \"Year\", \"Country\") #> Error in setDT(data): could not find function \"setDT\" dim(tensor) #> Error in eval(expr, envir, enclos): object 'tensor' not found"},{"path":"/reference/dict_size.html","id":null,"dir":"Reference","previous_headings":"","what":"Get cardinalities of the categorical variables — dict_size","title":"Get cardinalities of the categorical variables — dict_size","text":"Needed input [layer_multi_embedding()]","code":""},{"path":"/reference/dict_size.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get cardinalities of the categorical variables — dict_size","text":"","code":"dict_size(data, categorical)"},{"path":"/reference/dict_size.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get cardinalities of the categorical variables — dict_size","text":"data data.frame object categorical List categorical variables","code":""},{"path":"/reference/dict_size.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get cardinalities of the categorical variables — dict_size","text":"","code":"dict_size(m5::tiny_m5, c('event_name_1', 'event_type_1')) #> event_name_1 event_type_1  #>           31            5"},{"path":"/reference/layer_glu.html","id":null,"dir":"Reference","previous_headings":"","what":"Gated Linear Unit — layer_glu","title":"Gated Linear Unit — layer_glu","text":"form introduced Language modeling gated convolutional networks Dauphin et al., used sequence processing tasks compared gating mechanism used LSTM layers. context time series processing explicitly proposed Temporal Fusion Transformer.","code":""},{"path":"/reference/layer_glu.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Gated Linear Unit — layer_glu","text":"","code":"layer_glu(object, units, activation = NULL, return_gate = FALSE, ...)"},{"path":"/reference/layer_glu.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Gated Linear Unit — layer_glu","text":"object compose new Layer instance . Typically Sequential model Tensor (e.g., returned layer_input()). return value depends object. object : missing NULL, Layer instance returned. Sequential model, model additional layer returned. Tensor, output tensor layer_instance(object) returned. units Positive integer, dimensionality output space. activation Name activation function use. specify anything, activation applied (ie. \"linear\" activation: (x) = x). return_gate Logical - return gate values. Default: FALSE","code":""},{"path":"/reference/layer_glu.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Gated Linear Unit — layer_glu","text":"Tensor shape (batch_size, ..., units). Optionally, can also return weights tensor identical shape.","code":""},{"path":"/reference/layer_glu.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Gated Linear Unit — layer_glu","text":"Computed according equation: $$GLU(\\gamma) = \\sigma(W\\gamma + b) \\odot (V\\gamma + c)$$","code":""},{"path":"/reference/layer_glu.html","id":"input-and-output-shapes","dir":"Reference","previous_headings":"","what":"Input and Output Shapes","title":"Gated Linear Unit — layer_glu","text":"Input shape: nD tensor shape: (batch_size, ..., input_dim). common situation 2D input shape (batch_size, input_dim). Output shape: nD tensor shape: (batch_size, ..., units). instance, 2D input shape (batch_size, input_dim), output shape (batch_size, unit).","code":""},{"path":"/reference/layer_glu.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Gated Linear Unit — layer_glu","text":"Y. N. Dauphin., et al. Language modeling gated convolutional networks.. International conference machine learning. PMLR (2017) B. Lim, S.O. Arik, N. Loeff, T. Pfiste, Temporal Fusion Transformers Interpretable Multi-horizon Time Series Forecasting(2020) Implementation PyTorch Jan Beitner Implementation PyTorch Playtika Research","code":""},{"path":"/reference/layer_glu.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Gated Linear Unit — layer_glu","text":"","code":"library(keras)  # ================================================================ #             SEQUENTIAL MODEL, NO GATE VALUES RETURNED # ================================================================  model <-   keras_model_sequential() %>%   layer_glu(10, input_shape = 30) #> Loaded Tensorflow version 2.10.0  model #> Model: \"sequential\" #> ________________________________________________________________________________ #>  Layer (type)                       Output Shape                    Param #      #> ================================================================================ #>  glu (GLU)                          (None, 10)                      620          #> ================================================================================ #> Total params: 620 #> Trainable params: 620 #> Non-trainable params: 0 #> ________________________________________________________________________________  output <- model(matrix(1, 32, 30)) dim(output) #> [1] 32 10 output[1,] #> tf.Tensor( #> [ 2.1087797   0.8962488   0.9860752   0.40339586  0.43323898  0.03338135 #>   0.09097924  0.08465131 -0.20680998 -0.04013731], shape=(10,), dtype=float32)  # ================================================================ #                     WITH GATE VALUES RETURNED # ================================================================  inp  <- layer_input(30) out  <- layer_glu(units = 10, return_gate = TRUE)(inp)  model <- keras_model(inp, out)  model #> Model: \"model\" #> ________________________________________________________________________________ #>  Layer (type)                       Output Shape                    Param #      #> ================================================================================ #>  input_1 (InputLayer)               [(None, 30)]                    0            #>  glu_1 (GLU)                        [(None, 10),                    620          #>                                      (None, 10)]                                 #> ================================================================================ #> Total params: 620 #> Trainable params: 620 #> Non-trainable params: 0 #> ________________________________________________________________________________  c(values, gate) %<-% model(matrix(1, 32, 30)) dim(values) #> [1] 32 10 dim(gate) #> [1] 32 10  values[1,] #> tf.Tensor( #> [ 0.01150953  1.2098747  -0.9742861   0.14355768  0.23615074 -1.5777993 #>  -0.5639367   0.21935886  0.7237006   0.68133146], shape=(10,), dtype=float32) gate[1,] #> tf.Tensor( #> [0.87007254 0.4328773  0.8018212  0.14223629 0.23275425 0.82246184 #>  0.62256974 0.1425812  0.75942385 0.20313096], shape=(10,), dtype=float32)"},{"path":"/reference/layer_grn.html","id":null,"dir":"Reference","previous_headings":"","what":"Gated Residual Network block — layer_grn","title":"Gated Residual Network block — layer_grn","text":"GRN one elements TFT model composed . expected benefit applying value better ability switching linear non-linear processing.","code":""},{"path":"/reference/layer_grn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Gated Residual Network block — layer_grn","text":"","code":"layer_grn(   object,   hidden_units,   output_size = hidden_units,   dropout_rate = NULL,   use_context = FALSE,   return_gate = FALSE,   ... )"},{"path":"/reference/layer_grn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Gated Residual Network block — layer_grn","text":"object compose new Layer instance . Typically Sequential model Tensor (e.g., returned layer_input()). return value depends object. object : missing NULL, Layer instance returned. Sequential model, model additional layer returned. Tensor, output tensor layer_instance(object) returned. hidden_units Size hidden layer. output_size Dimensionality output feature space. use_context Use additional (static) context. TRUE, additional layer created handle context input. return_gate Logical - return gate values. Default: FALSE","code":""},{"path":"/reference/layer_grn.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Gated Residual Network block — layer_grn","text":"output computed : $$GRN(,c) = LayerNorm(+ GLU({\\eta}_1))$$ $${\\eta}_1 = W_1\\eta_2 + b_1$$ $$\\eta_2 = ELU(W_2a + W_3c + b_2)$$","code":""},{"path":"/reference/layer_grn.html","id":"input-and-output-shapes","dir":"Reference","previous_headings":"","what":"Input and Output Shapes","title":"Gated Residual Network block — layer_grn","text":"Input shape: nD tensor shape: (batch_size, ..., input_dim). common situation 2D input shape (batch_size, input_dim). Output shape: nD tensor shape: (batch_size, ..., units). instance, 2D input shape (batch_size, input_dim), output shape (batch_size, unit).","code":""},{"path":"/reference/layer_grn.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Gated Residual Network block — layer_grn","text":"B. Lim, S.O. Arik, N. Loeff, T. Pfiste, Temporal Fusion Transformers Interpretable Multi-horizon Time Series Forecasting(2020)","code":""},{"path":"/reference/layer_grn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Gated Residual Network block — layer_grn","text":"","code":"library(keras)  # ================================================================ #             SEQUENTIAL MODEL, NO GATE VALUES RETURNED # ================================================================  model <-   keras_model_sequential() %>%   layer_grn(10, input_shape = 30)  model #> Model: \"sequential_1\" #> ________________________________________________________________________________ #>  Layer (type)                       Output Shape                    Param #      #> ================================================================================ #>  grn (GRN)                          (None, 10)                      970          #> ================================================================================ #> Total params: 970 #> Trainable params: 970 #> Non-trainable params: 0 #> ________________________________________________________________________________  output <- model(matrix(1, 32, 30)) dim(output) #> [1] 32 10 output[1,] #> tf.Tensor( #> [-0.10964501 -0.6568189   0.4000288  -1.2055001   1.3044196  -1.7505759 #>   0.4213798  -0.64255935  1.3064632   0.9328084 ], shape=(10,), dtype=float32)  #'================================================================ #            WITH GATE VALUES AND ADDITIONAL CONTEXT # ================================================================  inp  <- layer_input(c(28, 5)) ctx  <- layer_input(10) out  <- layer_grn(             hidden_units = 10,             return_gate = TRUE,             use_context = TRUE          )(inp, context = ctx)  model <- keras_model(list(inp, ctx), out)  model #> Model: \"model_1\" #> ________________________________________________________________________________ #>  Layer (type)             Output Shape      Param #  Connected to                #> ================================================================================ #>  input_2 (InputLayer)     [(None, 28, 5)]   0        []                          #>  input_3 (InputLayer)     [(None, 10)]      0        []                          #>  grn_1 (GRN)              [(None, 28, 10),  570      ['input_2[0][0]',           #>                            (None, 28, 10)]            'input_3[0][0]']           #> ================================================================================ #> Total params: 570 #> Trainable params: 570 #> Non-trainable params: 0 #> ________________________________________________________________________________  arr_1 <- array(1, dim = c(1, 28, 5)) arr_2 <- array(1, dim = c(1, 10))  c(values, gate) %<-% model(list(arr_1, arr_2)) dim(values) #> [1]  1 28 10 dim(gate) #> [1]  1 28 10  values[1, all_dims()] #> Error in all_dims(): could not find function \"all_dims\" gate[1, all_dims()] #> Error in all_dims(): could not find function \"all_dims\""},{"path":"/reference/layer_interpretable_mh_attention.html","id":null,"dir":"Reference","previous_headings":"","what":"Interpretable multi-head attention layer — layer_interpretable_mh_attention","title":"Interpretable multi-head attention layer — layer_interpretable_mh_attention","text":"Interpretable multi-head attention layer","code":""},{"path":"/reference/layer_interpretable_mh_attention.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Interpretable multi-head attention layer — layer_interpretable_mh_attention","text":"","code":"layer_interpretable_mh_attention(   object,   state_size,   num_heads,   dropout_rate = 0,   ... )"},{"path":"/reference/layer_interpretable_mh_attention.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Interpretable multi-head attention layer — layer_interpretable_mh_attention","text":"num_heads Number attention heads. dropout_rate Dropout rate","code":""},{"path":"/reference/layer_interpretable_mh_attention.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Interpretable multi-head attention layer — layer_interpretable_mh_attention","text":"B. Lim, S.O. Arik, N. Loeff, T. Pfiste, Temporal Fusion Transformers Interpretable Multi-horizon Time Series Forecasting(2020) TFT original implementation Google","code":""},{"path":"/reference/layer_interpretable_mh_attention.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Interpretable multi-head attention layer — layer_interpretable_mh_attention","text":"","code":"lookback   <- 28 horizon    <- 14 all_steps  <- lookback + horizon state_size <- 5  queries <- layer_input(c(horizon, state_size)) keys    <- layer_input(c(all_steps, state_size)) values  <- layer_input(c(all_steps, state_size))  imh_attention <-    layer_interpretable_mh_attention(       state_size = state_size, num_heads = 10    )(queries, keys, values)"},{"path":"/reference/layer_lmu.html","id":null,"dir":"Reference","previous_headings":"","what":"Legendre Memory Unit layer — layer_lmu","title":"Legendre Memory Unit layer — layer_lmu","text":"layer trainable low-dimensional delay systems. unit buffers encoded input internally representing low-dimensional (.e., compressed) version sliding window. Nonlinear decodings representation, expressed B matrices, provide computations across window, derivative, energy, median value, etc (1, 2). Note decoder matrices can span across units input sequence.","code":""},{"path":"/reference/layer_lmu.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Legendre Memory Unit layer — layer_lmu","text":"","code":"layer_lmu(   object,   memory_d,   order,   theta,   hidden_cell,   trainable_theta = FALSE,   hidden_to_memory = FALSE,   memory_to_memory = FALSE,   input_to_hidden = FALSE,   discretizer = \"zoh\",   kernel_initializer = \"glorot_uniform\",   recurrent_initializer = \"orthogonal\",   kernel_regularizer = NULL,   recurrent_regularizer = NULL,   use_bias = FALSE,   bias_initializer = \"zeros\",   bias_regularizer = NULL,   dropout = 0,   recurrent_dropout = 0,   return_sequences = FALSE,   ... )"},{"path":"/reference/layer_lmu.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Legendre Memory Unit layer — layer_lmu","text":"memory_d Dimensionality input memory component. order number degrees transfer function LTI system used represent sliding window history. parameter sets number Legendre polynomials used orthogonally represent sliding window. theta number timesteps sliding window represented using LTI system. context, sliding window represents dynamic range data, fixed size, used predict value next time step. value smaller size input sequence, number steps represented time prediction, however entire sequence still processed order information projected hidden layer. trainable_theta enabled, theta updated course training. hidden_cell Keras Layer/RNNCell implementing hidden component. trainable_theta TRUE, theta learnt course training. Otherwise, kept constant. hidden_to_memory TRUE, connect output hidden component back memory component (default FALSE). memory_to_memory TRUE, add learnable recurrent connection (addition static input_to_hidden TRUE, connect input directly hidden component (addition discretizer method used discretize B matrices LMU. Current options \"zoh\" (short Zero Order Hold) \"euler\". \"zoh\" accurate, training slower \"euler\" trainable_theta=TRUE. Note larger theta needed discretizing using \"euler\" (value larger 4*order recommended). kernel_initializer Initializer weights input memory/hidden component. NULL, weights used, input size must match memory/hidden size. recurrent_initializer Initializer memory_to_memory weights (connection enabled). kernel_regularizer Regularizer weights input memory/hidden component. recurrent_regularizer Regularizer memory_to_memory weights (connection enabled). use_bias TRUE, memory component includes bias term. bias_initializer Initializer memory component bias term. used use_bias=TRUE. bias_regularizer Regularizer memory component bias term. used use_bias=TRUE. dropout Dropout rate input connections. recurrent_dropout Dropout rate memory_to_memory connection. return_sequences TRUE, return full output sequence. Otherwise, return just last output output sequence.","code":""},{"path":"/reference/layer_lmu.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Legendre Memory Unit layer — layer_lmu","text":"Voelker, Kajic . Eliasmith, Legendre Memory Units: Continuous-Time Representation Recurrent Neural Networks Voelker Eliasmith (2018). Improving spiking dynamical networks: Accurate delays, higher-order synapses, time cells. Neural Computation, 30(3): 569-609. Voelker Eliasmith. \"Methods systems implementing dynamic neural networks.\" U.S. Patent Application . 15/243,223. LSTM (Long Short-Term Memory) dead?, CrossValidated","code":""},{"path":"/reference/layer_lmu.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Legendre Memory Unit layer — layer_lmu","text":"","code":"if (FALSE) { library(keras) inp <- layer_input(c(28, 3)) hidden_cell <- layer_lstm_cell(10) lmu <- layer_lmu(memory_d=10, order=3, theta=28, hidden_cell=hidden_cell)(inp) model <- keras_model(inp, lmu) model(array(1, c(32, 28, 3))) }"},{"path":"/reference/layer_multi_dense.html","id":null,"dir":"Reference","previous_headings":"","what":"Multiple dense layers in one layer — layer_multi_dense","title":"Multiple dense layers in one layer — layer_multi_dense","text":"Multiple dense layers one layer","code":""},{"path":"/reference/layer_multi_dense.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Multiple dense layers in one layer — layer_multi_dense","text":"","code":"layer_multi_dense(object, units, new_dim = FALSE, ...)"},{"path":"/reference/layer_multi_dense.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Multiple dense layers in one layer — layer_multi_dense","text":"object compose new Layer instance . Typically Sequential model Tensor (e.g., returned layer_input()). return value depends object. object : missing NULL, Layer instance returned. Sequential model, model additional layer returned. Tensor, output tensor layer_instance(object) returned. units Positive integer, dimensionality output space.","code":""},{"path":"/reference/layer_multi_dense.html","id":"input-and-output-shapes","dir":"Reference","previous_headings":"","what":"Input and Output Shapes","title":"Multiple dense layers in one layer — layer_multi_dense","text":"Input shape: nD tensor shape: (batch_size, ..., input_dim). common situation 2D input shape (batch_size, input_dim). Output shape: length units equals 1 nD tensor shape: (batch_size, ..., units). instance, 2D input shape (batch_size, input_dim), output shape (batch_size, unit). length units greater 1 nD tensor shape: (batch_size, ..., units). instance, 2D input shape (batch_size, input_dim), output shape (batch_size, unit).","code":""},{"path":"/reference/layer_multi_dense.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Multiple dense layers in one layer — layer_multi_dense","text":"","code":"# ========================================================================== #                          SIMPLE CONCATENATION # ==========================================================================  inp <- layer_input(c(28, 3)) md <- layer_multi_dense(units = c(4, 6, 8))(inp)  md_model <- keras_model(inp, md)  dummy_input <- array(1, dim = c(1, 28, 3))  out <- md_model(dummy_input) dim(out) #> [1]  1 28 18  # ========================================================================== #                          NEW DIMESNION # ==========================================================================  inp <- layer_input(c(28, 3)) md <- layer_multi_dense(units = 5, new_dim = TRUE)(inp)  md_model <- keras_model(inp, md)  dummy_input <- array(1, dim = c(1, 28, 3))  out <- md_model(dummy_input) dim(out) #> [1]  1 28  3  5"},{"path":"/reference/layer_multi_embedding.html","id":null,"dir":"Reference","previous_headings":"","what":"Multiple embeddings in one layer — layer_multi_embedding","title":"Multiple embeddings in one layer — layer_multi_embedding","text":"Multiple embeddings one layer","code":""},{"path":"/reference/layer_multi_embedding.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Multiple embeddings in one layer — layer_multi_embedding","text":"","code":"layer_multi_embedding(object, input_dims, output_dims, new_dim = FALSE, ...)"},{"path":"/reference/layer_multi_embedding.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Multiple embeddings in one layer — layer_multi_embedding","text":"object compose new Layer instance . Typically Sequential model Tensor (e.g., returned layer_input()). return value depends object. object : missing NULL, Layer instance returned. Sequential model, model additional layer returned. Tensor, output tensor layer_instance(object) returned. new_dim TRUE, new dimension created instead stacking outputs dimension","code":""},{"path":"/reference/layer_multi_embedding.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Multiple embeddings in one layer — layer_multi_embedding","text":"","code":"# ========================================================================== #                          SIMPLE CONCATENATION # ==========================================================================  inp <- layer_input(c(28, 3)) emb <- layer_multi_embedding(input_dims = c(4, 6, 8), output_dims = c(3, 4, 5))(inp)  emb_model <- keras_model(inp, emb)  dummy_input <- array(1, dim = c(1, 28, 3)) dummy_input[,,1] <- sample(4,size = 28, replace = TRUE) dummy_input[,,2] <- sample(6,size = 28, replace = TRUE) dummy_input[,,3] <- sample(8,size = 28, replace = TRUE)  out <- emb_model(dummy_input) dim(out) #> [1]  1 28 12 # ========================================================================== #                              ONE VARIABLE # ==========================================================================  inp <- layer_input(c(32, 1)) emb <- layer_multi_embedding(input_dims = 10, output_dims = 2)(inp)  # ========================================================================== #                              NEW DIMESNION # ==========================================================================  inp <- layer_input(c(28, 3)) emb <- layer_multi_embedding(input_dims = c(4, 6, 8), output_dims = 5, new_dim = TRUE)(inp)  emb_model <- keras_model(inp, emb)  dummy_input <- array(1, dim = c(1, 28, 3)) dummy_input[,,1] <- sample(4,size = 28, replace = TRUE) dummy_input[,,2] <- sample(6,size = 28, replace = TRUE) dummy_input[,,3] <- sample(8,size = 28, replace = TRUE)  out <- emb_model(dummy_input) dim(out) #> [1]  1 28  3  5"},{"path":"/reference/layer_scaled_dot_attention.html","id":null,"dir":"Reference","previous_headings":"","what":"Scaled dot product attention layer — layer_scaled_dot_attention","title":"Scaled dot product attention layer — layer_scaled_dot_attention","text":"Introduced Attention Need. Defined :","code":""},{"path":"/reference/layer_scaled_dot_attention.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Scaled dot product attention layer — layer_scaled_dot_attention","text":"","code":"layer_scaled_dot_attention(object, dropout_rate = 0, ...)"},{"path":"/reference/layer_scaled_dot_attention.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Scaled dot product attention layer — layer_scaled_dot_attention","text":"dropout_rate Dropout rate","code":""},{"path":"/reference/layer_scaled_dot_attention.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Scaled dot product attention layer — layer_scaled_dot_attention","text":"$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$ Originally, dropout specified . added inside layer Temporal Fusion Transformer implementation Google. component Multi-Head Attention Layers (well interpretable version, available aion package).","code":""},{"path":"/reference/layer_scaled_dot_attention.html","id":"call-arguments","dir":"Reference","previous_headings":"","what":"Call arguments","title":"Scaled dot product attention layer — layer_scaled_dot_attention","text":"query: Query Tensor shape [B, T, dim]. value: Value Tensor shape [B, S, dim]. key: Optional key Tensor shape [B, S, dim]. given, use value key value, common case. attention_mask: boolean mask shape [B, T, S], prevents attention certain positions. return_attention_scores: boolean indicate whether output attention output TRUE, (attention_output, attention_scores) FALSE. Defaults FALSE. training: Python boolean indicating whether layer behave training mode (adding dropout) inference mode (dropout). Defaults either using training mode parent layer/model, FALSE (inference) parent layer.","code":""},{"path":"/reference/layer_scaled_dot_attention.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Scaled dot product attention layer — layer_scaled_dot_attention","text":". Vaswani et al. Attention Need(2017)  B. Lim, S.O. Arik, N. Loeff, T. Pfiste, Temporal Fusion Transformers Interpretable Multi-horizon Time Series Forecasting(2020)","code":""},{"path":"/reference/layer_scaled_dot_attention.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Scaled dot product attention layer — layer_scaled_dot_attention","text":"","code":"lookback   <- 28 horizon    <- 14 all_steps  <- lookback + horizon state_size <- 5  queries <- layer_input(c(horizon, state_size)) keys    <- layer_input(c(all_steps, state_size)) values  <- layer_input(c(all_steps, state_size))  sdp_attention <- layer_scaled_dot_attention()(queries, keys, values)"},{"path":"/reference/layer_tcn.html","id":null,"dir":"Reference","previous_headings":"","what":"Residual block for the WaveNet TCN — layer_tcn","title":"Residual block for the WaveNet TCN — layer_tcn","text":"block composed .. causal convolutional layers. may considered replacement recurrent layers.","code":""},{"path":"/reference/layer_tcn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Residual block for the WaveNet TCN — layer_tcn","text":"","code":"layer_tcn(   object,   nb_filters = 64,   kernel_size = 3,   nb_stacks = 1,   dilations = c(1, 7, 14),   padding = \"causal\",   use_skip_connections = TRUE,   dropout_rate = 0,   return_sequences = FALSE,   activation = \"relu\",   kernel_initializer = \"he_normal\",   use_batch_norm = FALSE,   use_layer_norm = FALSE,   use_weight_norm = FALSE,   input_shape = NULL,   ... )"},{"path":"/reference/layer_tcn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Residual block for the WaveNet TCN — layer_tcn","text":"nb_filters number convolutional filters use block kernel_size size convolutional kernel padding padding used convolutional layers, '' 'causal'. dropout_rate Float 0 1. Fraction input units drop. activation final activation used o = Activation(x + F(x)) kernel_initializer Initializer kernel weights matrix (Conv1D). use_batch_norm Whether use batch normalization residual layers . use_layer_norm Whether use layer normalization residual layers . use_weight_norm Whether use weight normalization residual layers . dilation_rate dilation power 2 using residual block","code":""},{"path":"/reference/layer_tcn.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Residual block for the WaveNet TCN — layer_tcn","text":"Keras TCN library Philippe Rémy Bai Sh., Kolter J.Z., Koltun V., Empirical Evaluation Generic Convolutional Recurrent Networks Sequence Modeling, 2018","code":""},{"path":"/reference/layer_tcn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Residual block for the WaveNet TCN — layer_tcn","text":"","code":"# \\donttest{ inp <- layer_input(c(28, 3)) tcn <- layer_tcn()(inp) model <- keras_model(inp, tcn) model(array(1, c(32, 28, 3))) #> tf.Tensor( #> [[1.2380545 5.4867787 0.        ... 7.667675  1.3840262 4.999212 ] #>  [1.2380545 5.4867787 0.        ... 7.667675  1.3840262 4.999212 ] #>  [1.2380545 5.4867787 0.        ... 7.667675  1.3840262 4.999212 ] #>  ... #>  [1.2380545 5.4867787 0.        ... 7.667675  1.3840262 4.999212 ] #>  [1.2380545 5.4867787 0.        ... 7.667675  1.3840262 4.999212 ] #>  [1.2380545 5.4867787 0.        ... 7.667675  1.3840262 4.999212 ]], shape=(32, 64), dtype=float32) # }"},{"path":"/reference/layer_temporal_fusion_decoder.html","id":null,"dir":"Reference","previous_headings":"","what":"Temporal Fusion Decoder layer — layer_temporal_fusion_decoder","title":"Temporal Fusion Decoder layer — layer_temporal_fusion_decoder","text":"One blocks, TFT model consists . call, accepts two parameters: output LSTM static context LSTM output enriched static context features additionally processed ath end.","code":""},{"path":"/reference/layer_temporal_fusion_decoder.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Temporal Fusion Decoder layer — layer_temporal_fusion_decoder","text":"","code":"layer_temporal_fusion_decoder(   object,   hidden_units,   state_size,   dropout_rate = 0,   use_context,   num_heads,   ... )"},{"path":"/reference/layer_temporal_fusion_decoder.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Temporal Fusion Decoder layer — layer_temporal_fusion_decoder","text":"","code":"lookback   <- 28 horizon    <- 14 all_steps  <- lookback + horizon state_size <- 5  lstm_output <- layer_input(c(all_steps, state_size)) context     <- layer_input(state_size)  # No attentiion scores returned tdf <- layer_temporal_fusion_decoder(    hidden_units = 30,    state_size = state_size,    use_context = TRUE,    num_heads = 10 )(lstm_output, context)  # With attention scores c(tfd, attention_scores) %<-%    layer_temporal_fusion_decoder(       hidden_units = 30,       state_size = state_size,       use_context = TRUE,       num_heads = 10    )(lstm_output, context, return_attention_scores=TRUE)"},{"path":"/reference/layer_vsn.html","id":null,"dir":"Reference","previous_headings":"","what":"Variable Selection Network block — layer_vsn","title":"Variable Selection Network block — layer_vsn","text":"receives four-dimensional vector input case dynamic data (batch_size, timesteps, n_features, feature_dim)","code":""},{"path":"/reference/layer_vsn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Variable Selection Network block — layer_vsn","text":"","code":"layer_vsn(   object,   hidden_units,   state_size,   dropout_rate = NULL,   use_context = FALSE,   return_weights = FALSE,   ... )"},{"path":"/reference/layer_vsn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Variable Selection Network block — layer_vsn","text":"state_size Dimensionality feature space, common across model. name comes original paper also refer $$d_model$$ return_weights Return weights selection.","code":""},{"path":"/reference/layer_vsn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Variable Selection Network block — layer_vsn","text":"tensor shapes: dynamic data - (batch_size, timesteps, state_size) static data - (batch_size, state_size)","code":""},{"path":"/reference/layer_vsn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Variable Selection Network block — layer_vsn","text":"","code":"# ========================================================================= #               THREE-DIMENSIONAL INPUT (STATIC FEATURES) # =========================================================================  # input: (batch_size, n_features, state_size)  inp <- layer_input(c(10, 5)) out <- layer_vsn(hidden_units = 10, state_size = 5)(inp) dim(out) #> [1] NA  5  # ========================================================================= #               FOUR-DIMENSIONAL INPUT (DYNAMIC FEATURES) # =========================================================================  # input: (batch_size, timesteps, n_features, state_size)  inp <- layer_input(c(28, 10, 5)) out <- layer_vsn(hidden_units = 10, state_size = 5)(inp) dim(out) #> [1] NA 28  5"},{"path":"/reference/loss_negative_log_likelihood.html","id":null,"dir":"Reference","previous_headings":"","what":"General negative log likelihood loss function — loss_negative_log_likelihood","title":"General negative log likelihood loss function — loss_negative_log_likelihood","text":"Bear mind, number model outputs must reflect number distribution parameters. example, use normal distribution (tfprobability::tfd_normal()), described two parameters (mean standard deviation), model return two values per timestep. othr words, produces distribution forecast rather point estimate. model trained, two options generate final forecast: use expected value distribution (e.g. mean normal distribution) sample value distribution Additionally, distribution can compute prediction intervals. Remeber also constraints imposed parameter values, e.g. standard deviation must positive.","code":""},{"path":"/reference/loss_negative_log_likelihood.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"General negative log likelihood loss function — loss_negative_log_likelihood","text":"","code":"loss_negative_log_likelihood(...)"},{"path":"/reference/loss_negative_log_likelihood.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"General negative log likelihood loss function — loss_negative_log_likelihood","text":"distribution probability distribution function tfprobability package. Default: tfprobability::tfd_normal()","code":""},{"path":"/reference/loss_negative_log_likelihood.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"General negative log likelihood loss function — loss_negative_log_likelihood","text":"Cross-Entropy, Negative Log-Likelihood, Jazz D. Salinas, V. Flunkert, J. Gasthaus, T. Januschowski, DeepAR: Probabilistic forecasting autoregressive recurrent networks, International Journal Forecasting(2019)","code":""},{"path":"/reference/loss_negative_log_likelihood.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"General negative log likelihood loss function — loss_negative_log_likelihood","text":"","code":"y_pred <- array(runif(60), c(2, 10, 2)) y_true <- array(runif(20), c(2, 10, 1))  loss_negative_log_likelihood(     distribution = tfprobability::tfd_normal,     reduction = 'auto'  )(y_true, y_pred) #> tf.Tensor(3.2511878, shape=(), dtype=float32) loss_negative_log_likelihood(reduction = 'sum')(y_true, y_pred) #> tf.Tensor(65.02376, shape=(), dtype=float32)"},{"path":"/reference/loss_quantile.html","id":null,"dir":"Reference","previous_headings":"","what":"Quantile (Pinball) loss function — loss_quantile","title":"Quantile (Pinball) loss function — loss_quantile","text":"generalized version quantile loss. model can predict multiple quantiles .","code":""},{"path":"/reference/loss_quantile.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Quantile (Pinball) loss function — loss_quantile","text":"","code":"loss_quantile(...)  loss_pinball(...)"},{"path":"/reference/loss_quantile.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Quantile (Pinball) loss function — loss_quantile","text":"quantiles List quantiles (numeric vector values 0 1).","code":""},{"path":"/reference/loss_quantile.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Quantile (Pinball) loss function — loss_quantile","text":"Loss value single sample-timestep-quantile computed : $$QL(y_t, \\hat{y}_t, q) = max(q(y_t - \\hat{y}_t), (q - 1)(y_t - \\hat{y}_t))$$ equivalently : $$QL(y_t, \\hat{y}_t, q) = max(q(y_t - \\hat{y}_t), 0) +  max((1 - q)(\\hat{y}_t - y_t), 0)$$ multiple quantiles defined, generalized, averaged loss computed according equation: $$\\mathcal{L}(\\Omega, W) = \\Sigma_{y_t \\\\Omega}\\Sigma_{q \\\\mathcal{Q}}\\Sigma^{\\tau_{max}}_{\\tau=1} \\frac{QL(y_t, \\hat{y}(q, t - \\tau, \\tau), q)}{M_{\\tau_{max}}}$$ loss function computed reduction = 'auto reduction = 'mean'.","code":""},{"path":"/reference/loss_quantile.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Quantile (Pinball) loss function — loss_quantile","text":"moment, can use loss_quantile instantiate class call directly like loss keras. Please see: #1342 issue","code":""},{"path":"/reference/loss_quantile.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Quantile (Pinball) loss function — loss_quantile","text":"B. Lim, S.O. Arik, N. Loeff, T. Pfiste, Temporal Fusion Transformers Interpretable Multi-horizon Time Series Forecasting(2020) Quantile loss function machine learning Pinball loss function (Lokad) Original TFT implementation Google","code":""},{"path":[]},{"path":"/reference/loss_quantile.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Quantile (Pinball) loss function — loss_quantile","text":"","code":"y_pred <- array(runif(60), c(2, 10, 3)) y_true <- array(runif(20), c(2, 10, 1))  loss_quantile(quantiles = c(0.1, 0.5, 0.9), reduction = 'auto')(y_true, y_pred) #> tf.Tensor(0.1629582, shape=(), dtype=float32) loss_quantile(quantiles = c(0.1, 0.5, 0.9), reduction = 'sum')(y_true, y_pred) #> tf.Tensor(9.777493, shape=(), dtype=float32)"},{"path":"/reference/loss_tweedie.html","id":null,"dir":"Reference","previous_headings":"","what":"Tweedie Loss (negative log likelihood) — loss_tweedie","title":"Tweedie Loss (negative log likelihood) — loss_tweedie","text":"Tweedie Loss (negative log likelihood)","code":""},{"path":"/reference/loss_tweedie.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tweedie Loss (negative log likelihood) — loss_tweedie","text":"","code":"loss_tweedie(...)"},{"path":"/reference/loss_tweedie.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tweedie Loss (negative log likelihood) — loss_tweedie","text":"p Power parameter 0, 2 range. allows choose desired distribution Tweedie distributions family.","code":""},{"path":"/reference/loss_tweedie.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Tweedie Loss (negative log likelihood) — loss_tweedie","text":"moment, can use loss_quantile instantiate class call directly like loss keras. Please see: #1342 issue","code":""},{"path":"/reference/loss_tweedie.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Tweedie Loss (negative log likelihood) — loss_tweedie","text":"Tweedie Loss Function p parameter","code":""},{"path":"/reference/loss_tweedie.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tweedie Loss (negative log likelihood) — loss_tweedie","text":"","code":"y_pred <- array(runif(60), c(2, 10, 1)) y_true <- array(runif(20), c(2, 10, 1))  loss_tweedie(p = 1.5, reduction = 'auto')(y_true, y_pred) #> tf.Tensor(3.194078538975908, shape=(), dtype=float64) loss_tweedie(p = 1.5, reduction = 'sum')(y_true, y_pred) #> tf.Tensor(63.881570779518164, shape=(), dtype=float64)"},{"path":"/reference/make_arrays.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare input/taget arrays for time series models — make_arrays","title":"Prepare input/taget arrays for time series models — make_arrays","text":"Prepare input/taget arrays time series models","code":""},{"path":"/reference/make_arrays.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare input/taget arrays for time series models — make_arrays","text":"","code":"make_arrays(   data,   key,   index,   lookback,   horizon,   stride = 1,   target,   numeric = NULL,   categorical = NULL,   static = NULL,   past = NULL,   future = NULL,   shuffle = TRUE,   sample_frac = 1,   y_past_sep = FALSE,   ... )"},{"path":"/reference/make_arrays.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare input/taget arrays for time series models — make_arrays","text":"data [data.table::data.table()] instance lookback length context past horizon forecast length stride Stride moving window target Target variable(s) numeric Numeric variables categorical Categorical variables static Static variables shuffle Shuffle samples. Set FALSE test dataset. y_past_sep Return past values target variable separate array. Typically, returned first feature X_past_num array. However, models (NBEATS) may easier processing keep values separate array.","code":""},{"path":"/reference/make_arrays.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare input/taget arrays for time series models — make_arrays","text":"list arrays. maximal possible content embraces eight arrays: y_past X_past_cat X_past_num y_fut X_fut_cat X_fut_num X_static_cat X_static_num","code":""},{"path":"/reference/make_arrays.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prepare input/taget arrays for time series models — make_arrays","text":"","code":"library(m5) library(recipes, warn.conflicts=FALSE) #> Loading required package: dplyr #>  #> Attaching package: ‘dplyr’ #> The following objects are masked from ‘package:stats’: #>  #>     filter, lag #> The following objects are masked from ‘package:base’: #>  #>     intersect, setdiff, setequal, union library(zeallot) library(dplyr, warn.conflicts=FALSE) library(data.table, warn.conflicts=FALSE)  # ========================================================================== #                          PREPARING THE DATA # ==========================================================================  train <- tiny_m5[date < '2016-01-01'] test  <- tiny_m5[date >= '2016-01-01']  m5_recipe <-    recipe(value ~ ., data=train) %>%    step_mutate(item_id_idx=item_id, store_id_idx=store_id) %>%    step_integer(item_id_idx, store_id_idx,                 wday, month,                 event_name_1, event_type_1,                 event_name_2, event_type_2,                 zero_based=TRUE) %>%    step_naomit(all_predictors()) %>%    prep()  train <- bake(m5_recipe, train) test  <- bake(m5_recipe, test)  TARGET      <- 'value' STATIC      <- c('item_id_idx', 'store_id_idx') CATEGORICAL <- c('event_name_1', 'event_type_1', STATIC) NUMERIC     <- c('sell_price', 'sell_price') KEY         <- c('item_id', 'store_id') INDEX       <- 'date' LOOKBACK    <- 28 HORIZON     <- 14 STRIDE      <- LOOKBACK  setDT(train) setDT(test)  # ========================================================================== #                           CREATING ARRAYS # ==========================================================================  train_arrays <-    make_arrays(        data        = train,        key         = KEY,        index       = INDEX,        lookback    = LOOKBACK,        horizon     = HORIZON,        stride      = STRIDE,        target      = TARGET,        static      = STATIC,        categorical = CATEGORICAL,        numeric     = NUMERIC    )  print(names(train_arrays)) #> [1] \"X_past_num\"   \"X_past_cat\"   \"y_fut\"        \"X_fut_num\"    \"X_fut_cat\"    #> [6] \"X_static_cat\" print(dim(train_arrays$X_past_num)) #> [1] 14758    28     3  test_arrays <-    make_arrays(        data        = train,        key         = KEY,        index       = INDEX,        lookback    = LOOKBACK,        horizon     = HORIZON,        stride      = STRIDE,        target      = TARGET,        static      = STATIC,        categorical = CATEGORICAL,        numeric     = NUMERIC    )  print(names(test_arrays)) #> [1] \"X_past_num\"   \"X_past_cat\"   \"y_fut\"        \"X_fut_num\"    \"X_fut_cat\"    #> [6] \"X_static_cat\" print(dim(test_arrays$X_past_num)) #> [1] 14758    28     3"},{"path":"/reference/metric_q_risk.html","id":null,"dir":"Reference","previous_headings":"","what":"q-Risk metric — metric_q_risk","title":"q-Risk metric — metric_q_risk","text":"Also referred : \\(\\pi-risk\\) (2) \\(p*-loss\\) (3) \\(ρ-quantile loss R_{ρ}\\) (4). metric based quantile loss. Loss value single sample-timestep-quantile computed : $$QL(y_t, \\hat{y}_t, q) = max(q(y_t - \\hat{y}_t), (q - 1)(y_t - \\hat{y}_t))$$ equivalently : $$QL(y_t, \\hat{y}_t, q) = max(q(y_t - \\hat{y}_t), 0) +  max((1 - q)(\\hat{y}_t - y_t), 0)$$ final form metric looks follows: $$q-Risk = \\frac{2\\Sigma_{y_t \\\\Omega}\\Sigma^{\\tau_{max}}_{\\tau=1}QL(y_t, \\hat{y}(q, t - \\tau, \\tau), q)}{\\Sigma_{y_t \\\\Omega}\\Sigma^{\\tau_{max}}_{\\tau=1}{|y_t|}}$$","code":""},{"path":"/reference/metric_q_risk.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"q-Risk metric — metric_q_risk","text":"","code":"metric_q_risk(...)"},{"path":"/reference/metric_q_risk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"q-Risk metric — metric_q_risk","text":"quantile desired quantile expressed numeric range 0, 1.","code":""},{"path":"/reference/metric_q_risk.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"q-Risk metric — metric_q_risk","text":"B. Lim, S.O. Arik, N. Loeff, T. Pfiste, Temporal Fusion Transformers Interpretable Multi-horizon Time Series Forecasting(2020) D. Salinas, V. Flunkert, J. Gasthaus, T. Januschowski, DeepAR: Probabilistic forecasting autoregressive recurrent networks, International Journal Forecasting(2019) S. S. Rangapuram, et al., Deep state space models time series forecasting, : NIPS(2018) S. Li, et al., Enhancing locality breaking memory bottleneck transformer time series forecasting, : NeurIPS(2019)","code":""},{"path":[]},{"path":"/reference/metric_q_risk.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"q-Risk metric — metric_q_risk","text":"","code":"y_pred <- array(runif(60), c(2, 10, 1)) y_true <- array(runif(20), c(2, 10, 1))  metric_q_risk(quantile=0.5)(y_pred, y_true) #> tf.Tensor(0.67805815, shape=(), dtype=float32) metric_q_risk(quantile=0.9)(y_pred, y_true) #> tf.Tensor(0.4858037, shape=(), dtype=float32)"},{"path":"/reference/model_tft.html","id":null,"dir":"Reference","previous_headings":"","what":"Temporal Fusion Transformer model — model_tft","title":"Temporal Fusion Transformer model — model_tft","text":"Temporal Fusion Transformer model","code":""},{"path":"/reference/model_tft.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Temporal Fusion Transformer model — model_tft","text":"","code":"model_tft(...)"},{"path":"/reference/model_tft.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Temporal Fusion Transformer model — model_tft","text":"lookback Number timesteps past horizon Forecast length (number timesteps) past_numeric_size Number numeric features past past_categorical_size Number categorical features past future_numeric_size Number numeric features future output_size Number models output. simple point estimate set 1.","code":""},{"path":"/reference/model_tft.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Temporal Fusion Transformer model — model_tft","text":"Paper Original TFT implementation TensorFlow clear implementation PyTorch","code":""},{"path":[]},{"path":"/reference/model_tft.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Temporal Fusion Transformer model — model_tft","text":"","code":"library(keras) library(aion)  tft <- model_tft(    lookback                = 28,    horizon                 = 14,    past_numeric_size       = 5,    past_categorical_size   = 2,    future_numeric_size     = 4,    future_categorical_size = 2,    vocab_static_size       = c(5, 5),    vocab_dynamic_size      = c(4, 4),    optimizer               = 'adam',    hidden_dim              = 12,    state_size              = 7,    num_heads                 = 10,    dropout_rate            = 0.1,    output_size             = 3    #quantiles               = 0.5 ) #> Error in py_call_impl(x, dots$args, dots$keywords): RuntimeError: Evaluation error: TypeError: ('Keyword argument not understood:', 'optimizer') #> .  X_static_cat <- array(sample(5, 32 * 2, replace=TRUE), c(32, 2)) - 1 X_static_num <- array(runif(32 * 1), c(32, 1))  X_past_num <- array(runif(32 * 28 * 2), c(32, 28, 2)) X_past_cat <- array(sample(4, 32 * 28 * 2, replace=TRUE), c(32, 28, 5))  X_fut_num <- array(runif(32 * 14 * 5), c(32, 28, 1)) X_fut_cat <- array(sample(4, 32 * 14 * 2, replace=TRUE), c(32, 28, 5))  tft(X_past_num, X_past_cat, X_fut_num, X_fut_cat, X_static_num, X_static_cat) #> Error in tft(X_past_num, X_past_cat, X_fut_num, X_fut_cat, X_static_num,     X_static_cat): could not find function \"tft\""},{"path":"/reference/safe_var.html","id":null,"dir":"Reference","previous_headings":"","what":"Var function, which accepts one-element vectors — safe_var","title":"Var function, which accepts one-element vectors — safe_var","text":"Var function, accepts one-element vectors","code":""},{"path":"/reference/safe_var.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Var function, which accepts one-element vectors — safe_var","text":"","code":"safe_var(x)"},{"path":"/reference/ts_generator.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a generator for time series data — ts_generator","title":"Create a generator for time series data — ts_generator","text":"advantage generator explicit arrays creation make_arrays beginning lower RAM space volume needed kind operation. full arrays created, allocate space examples selected passed data.frame. use ts_generator instead, following examples deliver batch sub-arrays created fly. means store examples RAM time.","code":""},{"path":"/reference/ts_generator.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a generator for time series data — ts_generator","text":"","code":"ts_generator(   data,   key,   index,   lookback,   horizon,   stride = 1,   target,   numeric = NULL,   categorical = NULL,   static = NULL,   past = NULL,   future = NULL,   shuffle = TRUE,   sample_frac = 1,   y_past_sep = FALSE,   batch_size = 1,   ... )"},{"path":"/reference/ts_generator.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a generator for time series data — ts_generator","text":"data [data.table::data.table()] instance lookback length context past horizon forecast length stride Stride moving window target Target variable(s) numeric Numeric variables categorical Categorical variables static Static variables shuffle Shuffle samples. Set FALSE test dataset. y_past_sep Return past values target variable separate array. Typically, returned first feature X_past_num array. However, models (NBEATS) may easier processing keep values separate array. batch_size Batch size","code":""},{"path":"/reference/ts_generator.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a generator for time series data — ts_generator","text":"","code":"library(m5) library(recipes, warn.conflicts=FALSE) library(zeallot) library(dplyr, warn.conflicts=FALSE) library(data.table, warn.conflicts=FALSE)  # ========================================================================== #                          PREPARING THE DATA # ==========================================================================  train <- tiny_m5[date < '2016-01-01'] test  <- tiny_m5[date >= '2016-01-01']  m5_recipe <-    recipe(value ~ ., data=train) %>%    step_mutate(item_id_idx=item_id, store_id_idx=store_id) %>%    step_integer(item_id_idx, store_id_idx,                 wday, month,                 event_name_1, event_type_1,                 event_name_2, event_type_2,                 zero_based=TRUE) %>%    step_naomit(all_predictors()) %>%    prep()  train <- bake(m5_recipe, train) test  <- bake(m5_recipe, test)  TARGET      <- 'value' STATIC      <- c('item_id_idx', 'store_id_idx') CATEGORICAL <- c('event_name_1', 'event_type_1', STATIC) NUMERIC     <- c('sell_price', 'sell_price') KEY         <- c('item_id', 'store_id') INDEX       <- 'date' LOOKBACK    <- 28 HORIZON     <- 14 STRIDE      <- LOOKBACK BATCH_SIZE  <- 32  setDT(train) setDT(test)  # ========================================================================== #                          CREATING GENERATORS # ==========================================================================  c(train_generator, train_steps) %<-%    ts_generator(        data = train,        key = KEY,        index = INDEX,        lookback = LOOKBACK,        horizon = HORIZON,        stride = STRIDE,        target=TARGET,        static=STATIC,        categorical=CATEGORICAL,        numeric=NUMERIC,        batch_size=BATCH_SIZE    )  batch <- train_generator() print(names(batch)) #> [1] \"X_past_num\"   \"X_past_cat\"   \"y_fut\"        \"X_fut_num\"    \"X_fut_cat\"    #> [6] \"X_static_cat\"  test_generator <-    ts_generator(        data = test,        key = KEY,        index = INDEX,        lookback = LOOKBACK,        horizon = HORIZON,        stride = STRIDE,        target=TARGET,        static=STATIC,        categorical=CATEGORICAL,        numeric=NUMERIC    )"}]
