[{"path":"/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2022 aion authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"/articles/preparing_input.html","id":"panel-data-m5","dir":"Articles","previous_headings":"","what":"Panel data (M5)","title":"Preparing input","text":"m5 package contains functions download preprocess data M5 Forecasting challenges. don’t need fetch whole dataset, ’ll settle subset named tiny_m5. tiny_m5 n example panel dataset. simply means contains multiple time series, distinguished item_id store_id. ’d like , optional preprocessing scaling, imputation etc. create set arrays (tensors) generator, serves respective batch-level tensors fly. Typically, time series forecasting field, can distiguish following types variables: past values target variable future values target variable past dynamic features (numeric categorical) future dynamic features (numeric categorical) static features Past target values can simply treated part tensor past dynamic features. simpliest possible scenario split dataset using certain date.","code":"head(tiny_m5) #>          item_id   dept_id  cat_id store_id state_id value       date wm_yr_wk #> 1: HOBBIES_1_330 HOBBIES_1 HOBBIES     CA_1       CA     0 2011-01-29    11101 #> 2: HOBBIES_1_330 HOBBIES_1 HOBBIES     CA_1       CA     0 2011-01-30    11101 #> 3: HOBBIES_1_330 HOBBIES_1 HOBBIES     CA_1       CA     0 2011-01-31    11101 #> 4: HOBBIES_1_330 HOBBIES_1 HOBBIES     CA_1       CA     0 2011-02-01    11101 #> 5: HOBBIES_1_330 HOBBIES_1 HOBBIES     CA_1       CA     2 2011-02-02    11101 #> 6: HOBBIES_1_330 HOBBIES_1 HOBBIES     CA_1       CA     4 2011-02-03    11101 #>      weekday wday month year event_name_1 event_type_1 event_name_2 #> 1:  Saturday    1     1 2011                                        #> 2:    Sunday    2     1 2011                                        #> 3:    Monday    3     1 2011                                        #> 4:   Tuesday    4     2 2011                                        #> 5: Wednesday    5     2 2011                                        #> 6:  Thursday    6     2 2011                                        #>    event_type_2 snap sell_price #> 1:                 0       7.44 #> 2:                 0       7.44 #> 3:                 0       7.44 #> 4:                 1       7.44 #> 5:                 1       7.44 #> 6:                 1       7.44 summary(tiny_m5$date) #>         Min.      1st Qu.       Median         Mean      3rd Qu.         Max.  #> \"2011-01-29\" \"2012-05-21\" \"2013-09-11\" \"2013-09-11\" \"2015-01-02\" \"2016-04-24\" PAST_FUTURE_SPLIT_DATE <- as.Date('2016-01-01')  past <- tiny_m5[date < PAST_FUTURE_SPLIT_DATE] future <- tiny_m5[date >= PAST_FUTURE_SPLIT_DATE]  ID          <- c('item_id', 'store_id') DATE        <- 'date' TARGET      <- 'value' NUMERIC     <- c('sell_price') CATEGORICAL <- c('wday', 'month', 'snap')  # X_past_numeric <- #   past %>%  #   select(!!c(TARGET, NUMERIC, DATE, ID)) %>%  #   as.data.table() %>%  #   as_3d_array(key = ID, index = DATE) #  # print(dim(X_past_numeric)) # # print(X_past_numeric[1, 1:10, 1:2]) #  # X_past_categorical <- #   past %>%  #   select(!!c(CATEGORICAL, DATE, ID)) %>%  #   as_3d_array(key = ID, index = DATE) #  # print(dim(X_past_categorical)) # #print(X_past_categorical[1, 1:10, 1:2]) #  # X_future_numeric <- #   past %>%  #   select(!!c(NUMERIC, DATE, ID)) %>%  #   as_3d_array(key = ID, index = DATE) #  # print(dim(X_past_categorical)) # #print(X_past_categorical[1, 1:10, 1:2]) #  # y <- #   future %>%  #   select(!!c(TARGET, DATE, ID)) %>%  #   as_3d_array(key = ID, index = DATE) #  # print(y)"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Krzysztof Joachimiak. Author, maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Joachimiak K (2022). aion: Time Series keras. R package version 0.1.0.","code":"@Manual{,   title = {aion: Time Series with keras},   author = {Krzysztof Joachimiak},   year = {2022},   note = {R package version 0.1.0}, }"},{"path":"/index.html","id":"aion","dir":"","previous_headings":"","what":"Time Series with keras","title":"Time Series with keras","text":"Temporal Fusion Transformer keras R","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Time Series with keras","text":"can install development version aion GitHub :","code":"# install.packages(\"devtools\") devtools::install_github(\"krzjoa/keras.tft\")"},{"path":"/index.html","id":"usage","dir":"","previous_headings":"","what":"Usage","title":"Time Series with keras","text":"","code":"# Dataset library(m5)  # Neural Networks library(aion) library(keras)  # Data wrangling library(recipes, warn.conflicts=FALSE) #> Ładowanie wymaganego pakietu: dplyr #>  #> Dołączanie pakietu: 'dplyr' #> Następujące obiekty zostały zakryte z 'package:stats': #>  #>     filter, lag #> Następujące obiekty zostały zakryte z 'package:base': #>  #>     intersect, setdiff, setequal, union library(dplyr, warn.conflicts=FALSE) library(data.table, warn.conflicts=FALSE)  # ========================================================================== #                          PREPARING THE DATA # ==========================================================================  train <- tiny_m5[date < '2016-01-01'] test  <- tiny_m5[date >= '2016-01-01']  m5_recipe <-   recipe(value ~ ., data=train) %>%   step_mutate(item_id_idx=item_id, store_id_idx=store_id) %>%   step_integer(item_id_idx, store_id_idx,                wday, month,                event_name_1, event_type_1,                event_name_2, event_type_2,                zero_based=TRUE) %>%   step_naomit(all_predictors()) %>%   prep()  train <- bake(m5_recipe, train) test  <- bake(m5_recipe, test)  TARGET      <- 'value' STATIC      <- c('item_id_idx', 'store_id_idx') CATEGORICAL <- c('event_name_1', 'event_type_1', STATIC) NUMERIC     <- c('sell_price', 'sell_price') KEY         <- c('item_id', 'store_id') INDEX       <- 'date' LOOKBACK    <- 28 HORIZON     <- 14 STRIDE      <- LOOKBACK BATCH_SIZE  <- 32  setDT(train) setDT(test)  # ========================================================================== #                          CREATING GENERATORS # ==========================================================================  c(train_generator, train_steps) %<-%     ts_generator(         data = train,         key = KEY,         index = INDEX,         lookback = LOOKBACK,         horizon = HORIZON,         stride = STRIDE,         target=TARGET,         static=STATIC,         categorical=CATEGORICAL,         numeric=NUMERIC,         batch_size=BATCH_SIZE       )  batch <- train_generator() print(names(batch)) #> [1] \"y_past\"       \"X_past_num\"   \"X_past_cat\"   \"y_fut\"        \"X_fut_num\"    #> [6] \"X_fut_cat\"    \"X_static_cat\"  test_generator <-     ts_generator(         data = test,         key = KEY,         index = INDEX,         lookback = LOOKBACK,         horizon = HORIZON,         stride = STRIDE,         target=TARGET,         static=STATIC,         categorical=CATEGORICAL,         numeric=NUMERIC     )"},{"path":[]},{"path":"/reference/aion-package.html","id":null,"dir":"Reference","previous_headings":"","what":"aion: Time Series Models with 'keras' — aion-package","title":"aion: Time Series Models with 'keras' — aion-package","text":"(maybe one line) Use four spaces indenting paragraphs within Description.","code":""},{"path":"/reference/aion-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"aion: Time Series Models with 'keras' — aion-package","text":"Krzysztof Joachimiak","code":""},{"path":"/reference/as_3d_array.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a 3-dimensional array out of a data.frame — as_3d_array","title":"Create a 3-dimensional array out of a data.frame — as_3d_array","text":"key index used create batch_size timesteps dimension respectively. end, excluded data.frame, present anymore final 3-dimensional array. time series lengths must equal.","code":""},{"path":"/reference/as_3d_array.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a 3-dimensional array out of a data.frame — as_3d_array","text":"","code":"as_3d_array(data, index, key)"},{"path":"/reference/as_3d_array.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a 3-dimensional array out of a data.frame — as_3d_array","text":"data data.frame object index time-related column key Columns, creates unique time series IDs","code":""},{"path":"/reference/as_3d_array.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a 3-dimensional array out of a data.frame — as_3d_array","text":"3-dimensional array whith following dimensions: (n_unique_ids, n_timesteps, n_features)","code":""},{"path":"/reference/as_3d_array.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a 3-dimensional array out of a data.frame — as_3d_array","text":"example, subset global_economy dataset {tibbledata} package, 263 countries (key), 58 years (index) two features assigned country/year pair, output array shape (263, 58, 2). Bear mind toy example, split dataset least two parts historical context past expected future target ML model. basic helper function -create complete input tensors, please use make_arrays() function.","code":""},{"path":"/reference/as_3d_array.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a 3-dimensional array out of a data.frame — as_3d_array","text":"","code":"library(tsibbledata) library(dplyr, warn.conflicts=FALSE)  # `global_economy` dataset comes from the `{tsibbledata}` package  selected_ge <-  global_economy %>%  select(Country, Year, Imports, Exports)  tensor <- as_3d_array(selected_ge, \"Year\", \"Country\") #> Error in setDT(data): could not find function \"setDT\" dim(tensor) #> Error in eval(expr, envir, enclos): object 'tensor' not found"},{"path":"/reference/layer_glu.html","id":null,"dir":"Reference","previous_headings":"","what":"Gated Linear Unit — layer_glu","title":"Gated Linear Unit — layer_glu","text":"form introduced Language modeling gated convolutional networks Dauphin et al., used sequence processing tasks compared gating mechanism used LSTM layers. context time series processing explicitly proposed Temporal Fusion Transformer.","code":""},{"path":"/reference/layer_glu.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Gated Linear Unit — layer_glu","text":"","code":"layer_glu(object, units, activation = NULL, return_gate = FALSE, ...)"},{"path":"/reference/layer_glu.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Gated Linear Unit — layer_glu","text":"object compose new Layer instance . Typically Sequential model Tensor (e.g., returned layer_input()). return value depends object. object : missing NULL, Layer instance returned. Sequential model, model additional layer returned. Tensor, output tensor layer_instance(object) returned. units Positive integer, dimensionality output space. activation Name activation function use. specify anything, activation applied (ie. \"linear\" activation: (x) = x). return_gate Logical - return gate values. Default: FALSE","code":""},{"path":"/reference/layer_glu.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Gated Linear Unit — layer_glu","text":"Tensor shape (batch_size, ..., units). Optionally, can also return weights tensor identical shape.","code":""},{"path":"/reference/layer_glu.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Gated Linear Unit — layer_glu","text":"Computed according equation: $$GLU(\\gamma) = \\sigma(W\\gamma + b) \\odot (V\\gamma + c)$$","code":""},{"path":"/reference/layer_glu.html","id":"input-and-output-shapes","dir":"Reference","previous_headings":"","what":"Input and Output Shapes","title":"Gated Linear Unit — layer_glu","text":"Input shape: nD tensor shape: (batch_size, ..., input_dim). common situation 2D input shape (batch_size, input_dim). Output shape: nD tensor shape: (batch_size, ..., units). instance, 2D input shape (batch_size, input_dim), output shape (batch_size, unit).","code":""},{"path":"/reference/layer_glu.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Gated Linear Unit — layer_glu","text":"Dauphin, Yann N., et al. (2017). Language modeling gated convolutional networks.. International conference machine learning. PMLR Lim, Bryan et al. (2019). Temporal Fusion Transformers Interpretable Multi-horizon Time Series Forecasting. arXiv Implementation PyTorch Jan Beitner Implementation PyTorch Playtika Research","code":""},{"path":"/reference/layer_glu.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Gated Linear Unit — layer_glu","text":"","code":"library(keras)  # ================================================================ #             SEQUENTIAL MODEL, NO GATE VALUES RETURNED # ================================================================  model <-   keras_model_sequential() %>%   layer_glu(10, input_shape = 30) #> Loaded Tensorflow version 2.7.0 #> Error in py_call_impl(callable, dots$args, dots$keywords): RuntimeError: in user code: #>  #>     File \"/home/krzysztof/R/x86_64-pc-linux-gnu-library/4.1/reticulate/python/rpytools/call.py\", line 21, in python_function  * #>         raise RuntimeError(res[kErrorKey]) #>  #>     RuntimeError: Evaluation error: tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized. #>     . #>   model #> Error in eval(expr, envir, enclos): object 'model' not found  output <- model(matrix(1, 32, 30)) #> Error in model(matrix(1, 32, 30)): could not find function \"model\" dim(output) #> Error in eval(expr, envir, enclos): object 'output' not found output[1,] #> Error in eval(expr, envir, enclos): object 'output' not found  # ================================================================ #                     WITH GATE VALUES RETURNED # ================================================================  inp  <- layer_input(30) out  <- layer_glu(units = 10, return_gate = TRUE)(inp) #> Error in py_call_impl(callable, dots$args, dots$keywords): RuntimeError: in user code: #>  #>     File \"/home/krzysztof/R/x86_64-pc-linux-gnu-library/4.1/reticulate/python/rpytools/call.py\", line 21, in python_function  * #>         raise RuntimeError(res[kErrorKey]) #>  #>     RuntimeError: Evaluation error: tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized. #>     . #>   model <- keras_model(inp, out) #> Error in py_resolve_dots(list(...)): object 'out' not found  model #> Error in eval(expr, envir, enclos): object 'model' not found  c(values, gate) %<-% model(matrix(1, 32, 30)) #> Error in model(matrix(1, 32, 30)): could not find function \"model\" dim(values) #> Error in eval(expr, envir, enclos): object 'values' not found dim(gate) #> Error in eval(expr, envir, enclos): object 'gate' not found  values[1,] #> Error in eval(expr, envir, enclos): object 'values' not found gate[1,] #> Error in eval(expr, envir, enclos): object 'gate' not found"},{"path":"/reference/layer_grn.html","id":null,"dir":"Reference","previous_headings":"","what":"Gated Residual Network block — layer_grn","title":"Gated Residual Network block — layer_grn","text":"GRN one elements TFT model composed . expected benefit applying value better ability switching linear non-linear processing.","code":""},{"path":"/reference/layer_grn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Gated Residual Network block — layer_grn","text":"","code":"layer_grn(   object,   hidden_units,   output_size = hidden_units,   dropout_rate = NULL,   use_context = FALSE,   return_gate = FALSE,   ... )"},{"path":"/reference/layer_grn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Gated Residual Network block — layer_grn","text":"object compose new Layer instance . Typically Sequential model Tensor (e.g., returned layer_input()). return value depends object. object : missing NULL, Layer instance returned. Sequential model, model additional layer returned. Tensor, output tensor layer_instance(object) returned. hidden_units Size hidden layer. output_size Dimensionality output feature space. use_context Use additional (static) context. TRUE, additional layer created handle context input. return_gate Logical - return gate values. Default: FALSE","code":""},{"path":"/reference/layer_grn.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Gated Residual Network block — layer_grn","text":"output computed : $$GRN(,c) = LayerNorm(+ GLU({\\eta}_1))$$ $${\\eta}_1 = W_1\\eta_2 + b_1$$ $$\\eta_2 = ELU(W_2a + W_3c + b_2)$$","code":""},{"path":"/reference/layer_grn.html","id":"input-and-output-shapes","dir":"Reference","previous_headings":"","what":"Input and Output Shapes","title":"Gated Residual Network block — layer_grn","text":"Input shape: nD tensor shape: (batch_size, ..., input_dim). common situation 2D input shape (batch_size, input_dim). Output shape: nD tensor shape: (batch_size, ..., units). instance, 2D input shape (batch_size, input_dim), output shape (batch_size, unit).","code":""},{"path":"/reference/layer_grn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Gated Residual Network block — layer_grn","text":"","code":"library(keras)  # ================================================================ #             SEQUENTIAL MODEL, NO GATE VALUES RETURNED # ================================================================  model <-   keras_model_sequential() %>%   layer_grn(10, input_shape = 30) #> Error in py_call_impl(callable, dots$args, dots$keywords): RuntimeError: in user code: #>  #>     File \"/home/krzysztof/R/x86_64-pc-linux-gnu-library/4.1/reticulate/python/rpytools/call.py\", line 21, in python_function  * #>         raise RuntimeError(res[kErrorKey]) #>  #>     RuntimeError: Evaluation error: tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized. #>     . #>   model #> Error in eval(expr, envir, enclos): object 'model' not found  output <- model(matrix(1, 32, 30)) #> Error in model(matrix(1, 32, 30)): could not find function \"model\" dim(output) #> Error in eval(expr, envir, enclos): object 'output' not found output[1,] #> Error in eval(expr, envir, enclos): object 'output' not found  #'================================================================ #            WITH GATE VALUES AND ADDITIONAL CONTEXT # ================================================================  inp  <- layer_input(c(28, 5)) ctx  <- layer_input(10) out  <- layer_grn(             hidden_units = 10,             return_gate = TRUE,             use_context = TRUE          )(inp, context = ctx) #> Error in py_call_impl(callable, dots$args, dots$keywords): RuntimeError: in user code: #>  #>     File \"/home/krzysztof/R/x86_64-pc-linux-gnu-library/4.1/reticulate/python/rpytools/call.py\", line 21, in python_function  * #>         raise RuntimeError(res[kErrorKey]) #>  #>     RuntimeError: Evaluation error: tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized. #>     . #>   model <- keras_model(list(inp, ctx), out) #> Error in py_resolve_dots(list(...)): object 'out' not found  model #> Error in eval(expr, envir, enclos): object 'model' not found  arr_1 <- array(1, dim = c(1, 28, 5)) arr_2 <- array(1, dim = c(1, 10))  c(values, gate) %<-% model(list(arr_1, arr_2)) #> Error in model(list(arr_1, arr_2)): could not find function \"model\" dim(values) #> Error in eval(expr, envir, enclos): object 'values' not found dim(gate) #> Error in eval(expr, envir, enclos): object 'gate' not found  values[1, all_dims()] #> Error in eval(expr, envir, enclos): object 'values' not found gate[1, all_dims()] #> Error in eval(expr, envir, enclos): object 'gate' not found"},{"path":"/reference/layer_interpretable_mh_attention.html","id":null,"dir":"Reference","previous_headings":"","what":"Interpretable multi-head attention layer — layer_interpretable_mh_attention","title":"Interpretable multi-head attention layer — layer_interpretable_mh_attention","text":"Interpretable multi-head attention layer","code":""},{"path":"/reference/layer_interpretable_mh_attention.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Interpretable multi-head attention layer — layer_interpretable_mh_attention","text":"","code":"layer_interpretable_mh_attention(   object,   state_size,   num_heads,   dropout_rate = 0,   ... )"},{"path":"/reference/layer_interpretable_mh_attention.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Interpretable multi-head attention layer — layer_interpretable_mh_attention","text":"num_heads Number attention heads. dropout_rate Dropout rate","code":""},{"path":"/reference/layer_interpretable_mh_attention.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Interpretable multi-head attention layer — layer_interpretable_mh_attention","text":"TFT original implementation Google","code":""},{"path":"/reference/layer_interpretable_mh_attention.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Interpretable multi-head attention layer — layer_interpretable_mh_attention","text":"","code":"lookback   <- 28 horizon    <- 14 all_steps  <- lookback + horizon state_size <- 5  queries <- layer_input(c(horizon, state_size)) keys    <- layer_input(c(all_steps, state_size)) values  <- layer_input(c(all_steps, state_size))  imh_attention <-    layer_interpretable_mh_attention(       state_size = state_size, num_heads = 10    )(queries, keys, values) #> Error in py_call_impl(callable, dots$args, dots$keywords): RuntimeError: in user code: #>  #>     File \"/home/krzysztof/R/x86_64-pc-linux-gnu-library/4.1/reticulate/python/rpytools/call.py\", line 21, in python_function  * #>         raise RuntimeError(res[kErrorKey]) #>  #>     RuntimeError: Evaluation error: tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized. #>     . #>"},{"path":"/reference/layer_lmu.html","id":null,"dir":"Reference","previous_headings":"","what":"Legendre Memory Unit layer — layer_lmu","title":"Legendre Memory Unit layer — layer_lmu","text":"layer trainable low-dimensional delay systems. unit buffers encoded input internally representing low-dimensional (.e., compressed) version sliding window. Nonlinear decodings representation, expressed B matrices, provide computations across window, derivative, energy, median value, etc (1, 2). Note decoder matrices can span across units input sequence.","code":""},{"path":"/reference/layer_lmu.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Legendre Memory Unit layer — layer_lmu","text":"","code":"layer_lmu(   object,   memory_d,   order,   theta,   hidden_cell,   trainable_theta = FALSE,   hidden_to_memory = FALSE,   memory_to_memory = FALSE,   input_to_hidden = FALSE,   discretizer = \"zoh\",   kernel_initializer = \"glorot_uniform\",   recurrent_initializer = \"orthogonal\",   kernel_regularizer = NULL,   recurrent_regularizer = NULL,   use_bias = FALSE,   bias_initializer = \"zeros\",   bias_regularizer = NULL,   dropout = 0,   recurrent_dropout = 0,   return_sequences = FALSE,   ... )"},{"path":"/reference/layer_lmu.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Legendre Memory Unit layer — layer_lmu","text":"memory_d Dimensionality input memory component. order number degrees transfer function LTI system used represent sliding window history. parameter sets number Legendre polynomials used orthogonally represent sliding window. theta number timesteps sliding window represented using LTI system. context, sliding window represents dynamic range data, fixed size, used predict value next time step. value smaller size input sequence, number steps represented time prediction, however entire sequence still processed order information projected hidden layer. trainable_theta enabled, theta updated course training. hidden_cell Keras Layer/RNNCell implementing hidden component. trainable_theta TRUE, theta learnt course training. Otherwise, kept constant. hidden_to_memory TRUE, connect output hidden component back memory component (default FALSE). memory_to_memory TRUE, add learnable recurrent connection (addition static input_to_hidden TRUE, connect input directly hidden component (addition discretizer method used discretize B matrices LMU. Current options \"zoh\" (short Zero Order Hold) \"euler\". \"zoh\" accurate, training slower \"euler\" trainable_theta=TRUE. Note larger theta needed discretizing using \"euler\" (value larger 4*order recommended). kernel_initializer Initializer weights input memory/hidden component. NULL, weights used, input size must match memory/hidden size. recurrent_initializer Initializer memory_to_memory weights (connection enabled). kernel_regularizer Regularizer weights input memory/hidden component. recurrent_regularizer Regularizer memory_to_memory weights (connection enabled). use_bias TRUE, memory component includes bias term. bias_initializer Initializer memory component bias term. used use_bias=TRUE. bias_regularizer Regularizer memory component bias term. used use_bias=TRUE. dropout Dropout rate input connections. recurrent_dropout Dropout rate memory_to_memory connection. return_sequences TRUE, return full output sequence. Otherwise, return just last output output sequence.","code":""},{"path":"/reference/layer_lmu.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Legendre Memory Unit layer — layer_lmu","text":"Voelker, Kajic . Eliasmith, Legendre Memory Units: Continuous-Time Representation Recurrent Neural Networks Voelker Eliasmith (2018). Improving spiking dynamical networks: Accurate delays, higher-order synapses, time cells. Neural Computation, 30(3): 569-609. Voelker Eliasmith. \"Methods systems implementing dynamic neural networks.\" U.S. Patent Application . 15/243,223. LSTM (Long Short-Term Memory) dead?, CrossValidated","code":""},{"path":"/reference/layer_lmu.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Legendre Memory Unit layer — layer_lmu","text":"","code":"# \\donttest{ library(keras) inp <- layer_input(c(28, 3)) hidden_cell <- layer_lstm_cell(10) lmu <- layer_lmu(memory_d=10, order=3, theta=28, hidden_cell=hidden_cell)(inp) #> Error in py_call_impl(callable, dots$args, dots$keywords): tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized. model <- keras_model(inp, lmu) #> Error in py_resolve_dots(list(...)): object 'lmu' not found model(array(1, c(32, 28, 3))) #> Error in model(array(1, c(32, 28, 3))): could not find function \"model\" # }"},{"path":"/reference/layer_multi_dense.html","id":null,"dir":"Reference","previous_headings":"","what":"Multiple dense layers in one layer — layer_multi_dense","title":"Multiple dense layers in one layer — layer_multi_dense","text":"Multiple dense layers one layer","code":""},{"path":"/reference/layer_multi_dense.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Multiple dense layers in one layer — layer_multi_dense","text":"","code":"layer_multi_dense(object, units, new_dim = FALSE, ...)"},{"path":"/reference/layer_multi_dense.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Multiple dense layers in one layer — layer_multi_dense","text":"object compose new Layer instance . Typically Sequential model Tensor (e.g., returned layer_input()). return value depends object. object : missing NULL, Layer instance returned. Sequential model, model additional layer returned. Tensor, output tensor layer_instance(object) returned. units Positive integer, dimensionality output space.","code":""},{"path":"/reference/layer_multi_dense.html","id":"input-and-output-shapes","dir":"Reference","previous_headings":"","what":"Input and Output Shapes","title":"Multiple dense layers in one layer — layer_multi_dense","text":"Input shape: nD tensor shape: (batch_size, ..., input_dim). common situation 2D input shape (batch_size, input_dim). Output shape: length units equals 1 nD tensor shape: (batch_size, ..., units). instance, 2D input shape (batch_size, input_dim), output shape (batch_size, unit). length units greater 1 nD tensor shape: (batch_size, ..., units). instance, 2D input shape (batch_size, input_dim), output shape (batch_size, unit).","code":""},{"path":"/reference/layer_multi_dense.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Multiple dense layers in one layer — layer_multi_dense","text":"","code":"# ========================================================================== #                          SIMPLE CONCATENATION # ==========================================================================  inp <- layer_input(c(28, 3)) md <- layer_multi_dense(units = c(4, 6, 8))(inp) #> Error in py_call_impl(callable, dots$args, dots$keywords): RuntimeError: Evaluation error: invalid first argument, must be vector (list or atomic).  md_model <- keras_model(inp, md) #> Error in py_resolve_dots(list(...)): object 'md' not found  dummy_input <- array(1, dim = c(1, 28, 3))  out <- md_model(dummy_input) #> Error in md_model(dummy_input): could not find function \"md_model\" dim(out) #> Error in eval(expr, envir, enclos): object 'out' not found  # ========================================================================== #                          NEW DIMESNION # ==========================================================================  inp <- layer_input(c(28, 3)) md <- layer_multi_dense(units = 5, new_dim = TRUE)(inp) #> Error in py_call_impl(callable, dots$args, dots$keywords): RuntimeError: in user code: #>  #>     File \"/home/krzysztof/R/x86_64-pc-linux-gnu-library/4.1/reticulate/python/rpytools/call.py\", line 21, in python_function  * #>         raise RuntimeError(res[kErrorKey]) #>  #>     RuntimeError: Evaluation error: tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized. #>     . #>   md_model <- keras_model(inp, md) #> Error in py_resolve_dots(list(...)): object 'md' not found  dummy_input <- array(1, dim = c(1, 28, 3))  out <- md_model(dummy_input) #> Error in md_model(dummy_input): could not find function \"md_model\" dim(out) #> Error in eval(expr, envir, enclos): object 'out' not found"},{"path":"/reference/layer_multi_embedding.html","id":null,"dir":"Reference","previous_headings":"","what":"Multiple embeddings in one layer — layer_multi_embedding","title":"Multiple embeddings in one layer — layer_multi_embedding","text":"Multiple embeddings one layer","code":""},{"path":"/reference/layer_multi_embedding.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Multiple embeddings in one layer — layer_multi_embedding","text":"","code":"layer_multi_embedding(object, input_dims, output_dims, new_dim = FALSE, ...)"},{"path":"/reference/layer_multi_embedding.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Multiple embeddings in one layer — layer_multi_embedding","text":"object compose new Layer instance . Typically Sequential model Tensor (e.g., returned layer_input()). return value depends object. object : missing NULL, Layer instance returned. Sequential model, model additional layer returned. Tensor, output tensor layer_instance(object) returned. new_dim TRUE, new dimension created instead stacking outputs dimension","code":""},{"path":"/reference/layer_multi_embedding.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Multiple embeddings in one layer — layer_multi_embedding","text":"","code":"# ========================================================================== #                          SIMPLE CONCATENATION # ==========================================================================  inp <- layer_input(c(28, 3)) emb <- layer_multi_embedding(input_dims = c(4, 6, 8), output_dims = c(3, 4, 5))(inp) #> Error in py_call_impl(callable, dots$args, dots$keywords): RuntimeError: in user code: #>  #>     File \"/home/krzysztof/R/x86_64-pc-linux-gnu-library/4.1/reticulate/python/rpytools/call.py\", line 21, in python_function  * #>         raise RuntimeError(res[kErrorKey]) #>  #>     RuntimeError: Evaluation error: tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized. #>     . #>   emb_model <- keras_model(inp, emb) #> Error in py_resolve_dots(list(...)): object 'emb' not found  dummy_input <- array(1, dim = c(1, 28, 3)) dummy_input[,,1] <- sample(4,size = 28, replace = TRUE) dummy_input[,,2] <- sample(6,size = 28, replace = TRUE) dummy_input[,,3] <- sample(8,size = 28, replace = TRUE)  out <- emb_model(dummy_input) #> Error in emb_model(dummy_input): could not find function \"emb_model\" dim(out) #> Error in eval(expr, envir, enclos): object 'out' not found # ========================================================================== #                              ONE VARIABLE # ==========================================================================  inp <- layer_input(c(32, 1)) emb <- layer_multi_embedding(input_dims = 10, output_dims = 2)(inp) #> Error in py_call_impl(callable, dots$args, dots$keywords): RuntimeError: in user code: #>  #>     File \"/home/krzysztof/R/x86_64-pc-linux-gnu-library/4.1/reticulate/python/rpytools/call.py\", line 21, in python_function  * #>         raise RuntimeError(res[kErrorKey]) #>  #>     RuntimeError: Evaluation error: tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized. #>     . #>   # ========================================================================== #                              NEW DIMESNION # ==========================================================================  inp <- layer_input(c(28, 3)) emb <- layer_multi_embedding(input_dims = c(4, 6, 8), output_dims = 5, new_dim = TRUE)(inp) #> Error in py_call_impl(callable, dots$args, dots$keywords): RuntimeError: in user code: #>  #>     File \"/home/krzysztof/R/x86_64-pc-linux-gnu-library/4.1/reticulate/python/rpytools/call.py\", line 21, in python_function  * #>         raise RuntimeError(res[kErrorKey]) #>  #>     RuntimeError: Evaluation error: tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized. #>     . #>   emb_model <- keras_model(inp, emb) #> Error in py_resolve_dots(list(...)): object 'emb' not found  dummy_input <- array(1, dim = c(1, 28, 3)) dummy_input[,,1] <- sample(4,size = 28, replace = TRUE) dummy_input[,,2] <- sample(6,size = 28, replace = TRUE) dummy_input[,,3] <- sample(8,size = 28, replace = TRUE)  out <- emb_model(dummy_input) #> Error in emb_model(dummy_input): could not find function \"emb_model\" dim(out) #> Error in eval(expr, envir, enclos): object 'out' not found"},{"path":"/reference/layer_scaled_dot_attention.html","id":null,"dir":"Reference","previous_headings":"","what":"Scaled dot product attention layer — layer_scaled_dot_attention","title":"Scaled dot product attention layer — layer_scaled_dot_attention","text":"Introduced Attention Need. Defined :","code":""},{"path":"/reference/layer_scaled_dot_attention.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Scaled dot product attention layer — layer_scaled_dot_attention","text":"","code":"layer_scaled_dot_attention(object, dropout_rate = 0, ...)"},{"path":"/reference/layer_scaled_dot_attention.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Scaled dot product attention layer — layer_scaled_dot_attention","text":"dropout_rate Dropout rate","code":""},{"path":"/reference/layer_scaled_dot_attention.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Scaled dot product attention layer — layer_scaled_dot_attention","text":"$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$ Originally, dropout specified . added inside layer Temporal Fusion Transformer implementation Google. component Multi-Head Attention Layers (well interpretable version, available aion package).","code":""},{"path":"/reference/layer_scaled_dot_attention.html","id":"call-arguments","dir":"Reference","previous_headings":"","what":"Call arguments","title":"Scaled dot product attention layer — layer_scaled_dot_attention","text":"query: Query Tensor shape [B, T, dim]. value: Value Tensor shape [B, S, dim]. key: Optional key Tensor shape [B, S, dim]. given, use value key value, common case. attention_mask: boolean mask shape [B, T, S], prevents attention certain positions. return_attention_scores: boolean indicate whether output attention output TRUE, (attention_output, attention_scores) FALSE. Defaults FALSE. training: Python boolean indicating whether layer behave training mode (adding dropout) inference mode (dropout). Defaults either using training mode parent layer/model, FALSE (inference) parent layer.","code":""},{"path":"/reference/layer_scaled_dot_attention.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Scaled dot product attention layer — layer_scaled_dot_attention","text":"Attention Need.","code":""},{"path":"/reference/layer_scaled_dot_attention.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Scaled dot product attention layer — layer_scaled_dot_attention","text":"","code":"lookback   <- 28 horizon    <- 14 all_steps  <- lookback + horizon state_size <- 5  queries <- layer_input(c(horizon, state_size)) keys    <- layer_input(c(all_steps, state_size)) values  <- layer_input(c(all_steps, state_size))  sdp_attention <- layer_scaled_dot_attention()(queries, keys, values)"},{"path":"/reference/layer_tcn.html","id":null,"dir":"Reference","previous_headings":"","what":"Residual block for the WaveNet TCN — layer_tcn","title":"Residual block for the WaveNet TCN — layer_tcn","text":"block composed .. causal convolutional layers. may considered replacement recurrent layers.","code":""},{"path":"/reference/layer_tcn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Residual block for the WaveNet TCN — layer_tcn","text":"","code":"layer_tcn(   object,   nb_filters = 64,   kernel_size = 3,   nb_stacks = 1,   dilations = c(1, 7, 14),   padding = \"causal\",   use_skip_connections = TRUE,   dropout_rate = 0,   return_sequences = FALSE,   activation = \"relu\",   kernel_initializer = \"he_normal\",   use_batch_norm = FALSE,   use_layer_norm = FALSE,   use_weight_norm = FALSE,   input_shape = NULL,   ... )"},{"path":"/reference/layer_tcn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Residual block for the WaveNet TCN — layer_tcn","text":"nb_filters number convolutional filters use block kernel_size size convolutional kernel padding padding used convolutional layers, '' 'causal'. dropout_rate Float 0 1. Fraction input units drop. activation final activation used o = Activation(x + F(x)) kernel_initializer Initializer kernel weights matrix (Conv1D). use_batch_norm Whether use batch normalization residual layers . use_layer_norm Whether use layer normalization residual layers . use_weight_norm Whether use weight normalization residual layers . dilation_rate dilation power 2 using residual block","code":""},{"path":"/reference/layer_tcn.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Residual block for the WaveNet TCN — layer_tcn","text":"Keras TCN library Philippe Rémy Bai Sh., Kolter J.Z., Koltun V., Empirical Evaluation Generic Convolutional Recurrent Networks Sequence Modeling, 2018","code":""},{"path":"/reference/layer_tcn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Residual block for the WaveNet TCN — layer_tcn","text":"","code":"# \\donttest{ inp <- layer_input(c(28, 3)) tcn <- layer_tcn()(inp) #> Error in py_call_impl(callable, dots$args, dots$keywords): tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized. model <- keras_model(inp, tcn) #> Error in py_resolve_dots(list(...)): object 'tcn' not found model(array(1, c(32, 28, 3))) #> Error in model(array(1, c(32, 28, 3))): could not find function \"model\" # }"},{"path":"/reference/layer_temporal_fusion_decoder.html","id":null,"dir":"Reference","previous_headings":"","what":"Temporal Fusion Decoder layer — layer_temporal_fusion_decoder","title":"Temporal Fusion Decoder layer — layer_temporal_fusion_decoder","text":"One blocks, TFT model consists . call, accepts two parameters: output LSTM static context LSTM output enriched static context features additionally processed ath end.","code":""},{"path":"/reference/layer_temporal_fusion_decoder.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Temporal Fusion Decoder layer — layer_temporal_fusion_decoder","text":"","code":"layer_temporal_fusion_decoder(   object,   hidden_units,   state_size,   dropout_rate = 0,   use_context,   num_heads,   ... )"},{"path":"/reference/layer_temporal_fusion_decoder.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Temporal Fusion Decoder layer — layer_temporal_fusion_decoder","text":"","code":"lookback   <- 28 horizon    <- 14 all_steps  <- lookback + horizon state_size <- 5  lstm_output <- layer_input(c(all_steps, state_size)) context     <- layer_input(state_size)  # No attentiion scores returned tdf <- layer_temporal_fusion_decoder(    hidden_units = 30,    state_size = state_size,    use_context = TRUE,    num_heads = 10 )(lstm_output, context) #> Error in py_call_impl(callable, dots$args, dots$keywords): RuntimeError: Exception encountered when calling layer \"temporal_fusion_decoder\" (type TemporalFusionDecoder). #> <... omitted ...>u-library/4.1/reticulate/python/rpytools/call.py\", line 21, in python_function  * #>             raise RuntimeError(res[kErrorKey]) #>      #>         RuntimeError: Evaluation error: tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized. #>         . #>      #>      #>     Call arguments received: #>       • self=tf.Tensor(shape=(None, 42, 5), dtype=float32) #>       • inputs=tf.Tensor(shape=(None, 5), dtype=float32) #>       • context=None #>     . #>  #>  #> Call arguments received: #>   • self=tf.Tensor(shape=(None, 42, 5), dtype=float32) #>   • inputs=tf.Tensor(shape=(None, 5), dtype=float32) #>   • context=None #>   • return_attention_scores=False #> See `reticulate::py_last_error()` for details  # With attention scores c(tfd, attention_scores) %<-%    layer_temporal_fusion_decoder(       hidden_units = 30,       state_size = state_size,       use_context = TRUE,       num_heads = 10    )(lstm_output, context, return_attention_scores=TRUE) #> Error in py_call_impl(callable, dots$args, dots$keywords): RuntimeError: Exception encountered when calling layer \"temporal_fusion_decoder_1\" (type TemporalFusionDecoder). #> <... omitted ...>-library/4.1/reticulate/python/rpytools/call.py\", line 21, in python_function  * #>             raise RuntimeError(res[kErrorKey]) #>      #>         RuntimeError: Evaluation error: tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized. #>         . #>      #>      #>     Call arguments received: #>       • self=tf.Tensor(shape=(None, 42, 5), dtype=float32) #>       • inputs=tf.Tensor(shape=(None, 5), dtype=float32) #>       • context=None #>     . #>  #>  #> Call arguments received: #>   • self=tf.Tensor(shape=(None, 42, 5), dtype=float32) #>   • inputs=tf.Tensor(shape=(None, 5), dtype=float32) #>   • context=None #>   • return_attention_scores=True #> See `reticulate::py_last_error()` for details"},{"path":"/reference/layer_vsn.html","id":null,"dir":"Reference","previous_headings":"","what":"Variable Selection Network block — layer_vsn","title":"Variable Selection Network block — layer_vsn","text":"receives four-dimensional vector input case dynamic data (batch_size, timesteps, n_features, feature_dim)","code":""},{"path":"/reference/layer_vsn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Variable Selection Network block — layer_vsn","text":"","code":"layer_vsn(   object,   hidden_units,   state_size,   dropout_rate = NULL,   use_context = FALSE,   return_weights = FALSE,   ... )"},{"path":"/reference/layer_vsn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Variable Selection Network block — layer_vsn","text":"state_size Dimensionality feature space, common across model. name comes original paper also refer $$d_model$$ return_weights Return weights selection.","code":""},{"path":"/reference/layer_vsn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Variable Selection Network block — layer_vsn","text":"tensor shapes: dynamic data - (batch_size, timesteps, state_size) static data - (batch_size, state_size)","code":""},{"path":"/reference/layer_vsn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Variable Selection Network block — layer_vsn","text":"","code":"# ========================================================================= #               THREE-DIMENSIONAL INPUT (STATIC FEATURES) # =========================================================================  # input: (batch_size, n_features, state_size)  inp <- layer_input(c(10, 5)) out <- layer_vsn(hidden_units = 10, state_size = 5)(inp) #> Error in py_call_impl(callable, dots$args, dots$keywords): RuntimeError: in user code: #>  #>     File \"/home/krzysztof/R/x86_64-pc-linux-gnu-library/4.1/reticulate/python/rpytools/call.py\", line 21, in python_function  * #>         raise RuntimeError(res[kErrorKey]) #>  #>     RuntimeError: Evaluation error: RuntimeError: Exception encountered when calling layer \"grn\" (type GRN). #>      #>     in user code: #>      #>         File \"/home/krzysztof/R/x86_64-pc-linux-gnu-library/4.1/reticulate/python/rpytools/call.py\", line 21, in python_function  * #>             raise RuntimeError(res[kErrorKey]) #>      #>         RuntimeError: Evaluation error: tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized. #>         . #>      #>      #>     Call arguments received: #>       • self=tf.Tensor(shape=(None, 50), dtype=float32) #>       • inputs=None #>       • context=None #>     . #>  dim(out) #> Error in eval(expr, envir, enclos): object 'out' not found  # ========================================================================= #               FOUR-DIMENSIONAL INPUT (DYNAMIC FEATURES) # =========================================================================  # input: (batch_size, timesteps, n_features, state_size)  inp <- layer_input(c(28, 10, 5)) out <- layer_vsn(hidden_units = 10, state_size = 5)(inp) #> Error in py_call_impl(callable, dots$args, dots$keywords): RuntimeError: in user code: #>  #>     File \"/home/krzysztof/R/x86_64-pc-linux-gnu-library/4.1/reticulate/python/rpytools/call.py\", line 21, in python_function  * #>         raise RuntimeError(res[kErrorKey]) #>  #>     RuntimeError: Evaluation error: RuntimeError: Exception encountered when calling layer \"grn\" (type GRN). #>      #>     in user code: #>      #>         File \"/home/krzysztof/R/x86_64-pc-linux-gnu-library/4.1/reticulate/python/rpytools/call.py\", line 21, in python_function  * #>             raise RuntimeError(res[kErrorKey]) #>      #>         RuntimeError: Evaluation error: tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized. #>         . #>      #>      #>     Call arguments received: #>       • self=tf.Tensor(shape=(None, 28, 50), dtype=float32) #>       • inputs=None #>       • context=None #>     . #>  dim(out) #> Error in eval(expr, envir, enclos): object 'out' not found"},{"path":"/reference/loss_negative_log_likelihood.html","id":null,"dir":"Reference","previous_headings":"","what":"General negative log likelihood loss function — loss_negative_log_likelihood","title":"General negative log likelihood loss function — loss_negative_log_likelihood","text":"General negative log likelihood loss function","code":""},{"path":"/reference/loss_negative_log_likelihood.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"General negative log likelihood loss function — loss_negative_log_likelihood","text":"","code":"loss_negative_log_likelihood(...)"},{"path":"/reference/loss_negative_log_likelihood.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"General negative log likelihood loss function — loss_negative_log_likelihood","text":"distribution probability distribution function tfprobability package.","code":""},{"path":"/reference/loss_negative_log_likelihood.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"General negative log likelihood loss function — loss_negative_log_likelihood","text":"","code":"y_pred <- array(runif(60), c(2, 10, 2)) y_true <- array(runif(20), c(2, 10, 1))  loss_negative_log_likelihood(reduction = 'auto')(y_true, y_pred) #> Error in py_call_impl(x, dots$args, dots$keywords): RuntimeError: Evaluation error: .onLoad failed in loadNamespace() for 'tfprobability', details: #>   call: py_module_import(module, convert = convert) #>   error: ImportError: This version of TensorFlow Probability requires TensorFlow version >= 2.8; Detected an installation of version 2.7.0. Please upgrade TensorFlow to proceed. #> . loss_negative_log_likelihood(reduction = 'sum')(y_true, y_pred) #> Error in py_call_impl(x, dots$args, dots$keywords): RuntimeError: Evaluation error: .onLoad failed in loadNamespace() for 'tfprobability', details: #>   call: py_module_import(module, convert = convert) #>   error: ImportError: This version of TensorFlow Probability requires TensorFlow version >= 2.8; Detected an installation of version 2.7.0. Please upgrade TensorFlow to proceed. #> ."},{"path":"/reference/loss_quantile.html","id":null,"dir":"Reference","previous_headings":"","what":"Quantile (Pinball) loss function — loss_quantile","title":"Quantile (Pinball) loss function — loss_quantile","text":"generalized version quantile loss. model can predict multiple quantiles .","code":""},{"path":"/reference/loss_quantile.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Quantile (Pinball) loss function — loss_quantile","text":"","code":"loss_quantile(...)  loss_pinball(...)"},{"path":"/reference/loss_quantile.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Quantile (Pinball) loss function — loss_quantile","text":"quantiles List quantiles (numeric vector values 0 1).","code":""},{"path":"/reference/loss_quantile.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Quantile (Pinball) loss function — loss_quantile","text":"Loss value single sample-timestep-quantile computed : $$QL(y_t, \\hat{y}_t, q) = max(q(y_t - \\hat{y}_t), (q - 1)(y_t - \\hat{y}_t))$$ equivalently : $$QL(y_t, \\hat{y}_t, q) = max(q(y_t - \\hat{y}_t), 0) +  max((1 - q)(\\hat{y}_t - y_t), 0)$$ multiple quantiles defined, generalized, averaged loss computed according equation: $$\\mathcal{L}(\\Omega, W) = \\Sigma_{y_t \\\\Omega}\\Sigma_{q \\\\mathcal{Q}}\\Sigma^{\\tau_{max}}_{\\tau=1} = \\frac{QL(y_t, \\hat{y}(q, t - \\tau, \\tau), q)}{M_{\\tau_{max}}}$$ loss function computed reduction = 'auto reduction = 'mean'.","code":""},{"path":"/reference/loss_quantile.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Quantile (Pinball) loss function — loss_quantile","text":"moment, can use loss_quantile instantiate class call directly like loss keras. Please see: #1342 issue","code":""},{"path":"/reference/loss_quantile.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Quantile (Pinball) loss function — loss_quantile","text":"Quantile loss function machine learning Pinball loss function (Lokad) Temporal Fusion Transformer Original TFT implementation Google","code":""},{"path":"/reference/loss_quantile.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Quantile (Pinball) loss function — loss_quantile","text":"","code":"y_pred <- array(runif(60), c(2, 10, 3)) y_true <- array(runif(20), c(2, 10, 1))  loss_quantile(quantiles = c(0.1, 0.5, 0.9), reduction = 'auto')(y_true, y_pred) #> Error in py_call_impl(callable, dots$args, dots$keywords): RuntimeError: Evaluation error: tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized. #> . loss_quantile(quantiles = c(0.1, 0.5, 0.9), reduction = 'sum')(y_true, y_pred) #> Error in py_call_impl(callable, dots$args, dots$keywords): RuntimeError: Evaluation error: tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized. #> ."},{"path":"/reference/loss_tweedie.html","id":null,"dir":"Reference","previous_headings":"","what":"Tweedie Loss (negative log likelihood) — loss_tweedie","title":"Tweedie Loss (negative log likelihood) — loss_tweedie","text":"Tweedie Loss (negative log likelihood)","code":""},{"path":"/reference/loss_tweedie.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tweedie Loss (negative log likelihood) — loss_tweedie","text":"","code":"loss_tweedie(...)"},{"path":"/reference/loss_tweedie.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tweedie Loss (negative log likelihood) — loss_tweedie","text":"p Power parameter 0, 2 range. allows choose desired distribution Tweedie distributions family.","code":""},{"path":"/reference/loss_tweedie.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Tweedie Loss (negative log likelihood) — loss_tweedie","text":"moment, can use loss_quantile instantiate class call directly like loss keras. Please see: #1342 issue","code":""},{"path":"/reference/loss_tweedie.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Tweedie Loss (negative log likelihood) — loss_tweedie","text":"Tweedie Loss Function p parameter","code":""},{"path":"/reference/loss_tweedie.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tweedie Loss (negative log likelihood) — loss_tweedie","text":"","code":"y_pred <- array(runif(60), c(2, 10, 1)) y_true <- array(runif(20), c(2, 10, 1))  loss_tweedie(p = 1.5, reduction = 'auto')(y_true, y_pred) #> Error in py_call_impl(callable, dots$args, dots$keywords): tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized. loss_tweedie(p = 1.5, reduction = 'sum')(y_true, y_pred) #> Error in py_call_impl(callable, dots$args, dots$keywords): tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."},{"path":"/reference/make_arrays.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare input/taget arrays for time series models — make_arrays","title":"Prepare input/taget arrays for time series models — make_arrays","text":"Prepare input/taget arrays time series models","code":""},{"path":"/reference/make_arrays.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare input/taget arrays for time series models — make_arrays","text":"","code":"make_arrays(   data,   key,   index,   lookback,   horizon,   stride = 1,   shuffle = TRUE,   sample_frac = 1,   target,   numeric = NULL,   categorical = NULL,   static = NULL,   past = NULL,   future = NULL,   ... )"},{"path":"/reference/make_arrays.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare input/taget arrays for time series models — make_arrays","text":"lookback length context past horizon forecast length stride Stride moving window shuffle Shuffle samples. Set FALSE test dataset. target Target variable(s) numeric Numeric variables categorical Categorical variables static Static variables","code":""},{"path":"/reference/make_arrays.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare input/taget arrays for time series models — make_arrays","text":"list arrays. maximal possible content embraces eight arrays: y_past X_past_cat X_past_num y_fut X_fut_cat X_fut_num X_static_cat X_static_num","code":""},{"path":"/reference/make_arrays.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prepare input/taget arrays for time series models — make_arrays","text":"","code":"library(m5) library(recipes, warn.conflicts=FALSE) #> Loading required package: dplyr #>  #> Attaching package: ‘dplyr’ #> The following objects are masked from ‘package:stats’: #>  #>     filter, lag #> The following objects are masked from ‘package:base’: #>  #>     intersect, setdiff, setequal, union library(zeallot) library(dplyr, warn.conflicts=FALSE) library(data.table, warn.conflicts=FALSE)  # ========================================================================== #                          PREPARING THE DATA # ==========================================================================  train <- tiny_m5[date < '2016-01-01'] test  <- tiny_m5[date >= '2016-01-01']  m5_recipe <-    recipe(value ~ ., data=train) %>%    step_mutate(item_id_idx=item_id, store_id_idx=store_id) %>%    step_integer(item_id_idx, store_id_idx,                 wday, month,                 event_name_1, event_type_1,                 event_name_2, event_type_2,                 zero_based=TRUE) %>%    step_naomit(all_predictors()) %>%    prep()  train <- bake(m5_recipe, train) test  <- bake(m5_recipe, test)  TARGET      <- 'value' STATIC      <- c('item_id_idx', 'store_id_idx') CATEGORICAL <- c('event_name_1', 'event_type_1', STATIC) NUMERIC     <- c('sell_price', 'sell_price') KEY         <- c('item_id', 'store_id') INDEX       <- 'date' LOOKBACK    <- 28 HORIZON     <- 14 STRIDE      <- LOOKBACK  setDT(train) setDT(test)  # ========================================================================== #                           CREATING ARRAYS # ==========================================================================  train_arrays <-    make_arrays(        data        = train,        key         = KEY,        index       = INDEX,        lookback    = LOOKBACK,        horizon     = HORIZON,        stride      = STRIDE,        target      = TARGET,        static      = STATIC,        categorical = CATEGORICAL,        numeric     = NUMERIC    )  print(names(train_arrays)) #> [1] \"y_past\"       \"X_past_num\"   \"X_past_cat\"   \"y_fut\"        \"X_fut_num\"    #> [6] \"X_fut_cat\"    \"X_static_cat\" print(dim(train_arrays$X_past_num)) #> [1] 14758    28     2  test_arrays <-    make_arrays(        data        = train,        key         = KEY,        index       = INDEX,        lookback    = LOOKBACK,        horizon     = HORIZON,        stride      = STRIDE,        target      = TARGET,        static      = STATIC,        categorical = CATEGORICAL,        numeric     = NUMERIC    )  print(names(test_arrays)) #> [1] \"y_past\"       \"X_past_num\"   \"X_past_cat\"   \"y_fut\"        \"X_fut_num\"    #> [6] \"X_fut_cat\"    \"X_static_cat\" print(dim(test_arrays$X_past_num)) #> [1] 14758    28     2"},{"path":"/reference/metric_q_risk.html","id":null,"dir":"Reference","previous_headings":"","what":"q-Risk metric — metric_q_risk","title":"q-Risk metric — metric_q_risk","text":"q-Risk metric","code":""},{"path":"/reference/metric_q_risk.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"q-Risk metric — metric_q_risk","text":"","code":"metric_q_risk(...)"},{"path":"/reference/model_tft.html","id":null,"dir":"Reference","previous_headings":"","what":"Temporal Fusion Transformer model — model_tft","title":"Temporal Fusion Transformer model — model_tft","text":"Temporal Fusion Transformer model","code":""},{"path":"/reference/model_tft.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Temporal Fusion Transformer model — model_tft","text":"","code":"model_tft(...)"},{"path":"/reference/model_tft.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Temporal Fusion Transformer model — model_tft","text":"lookback Number timesteps past horizon Forecast length (number timesteps) past_numeric_size Number numeric features past past_categorical_size Number categorical features past future_numeric_size Number numeric features future","code":""},{"path":"/reference/model_tft.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Temporal Fusion Transformer model — model_tft","text":"Paper Original TFT implementation TensorFlow clear implementation PyTorch","code":""},{"path":"/reference/model_tft.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Temporal Fusion Transformer model — model_tft","text":"","code":"library(keras) library(aion)  tft <- model_tft(    lookback                = 28,    horizon                 = 14,    past_numeric_size       = 5,    past_categorical_size   = 2,    future_numeric_size     = 4,    future_categorical_size = 2,    vocab_static_size       = c(5, 5),    vocab_dynamic_size      = c(4, 4),    optimizer               = 'adam',    hidden_dim              = 12,    state_size              = 7,    n_heads                 = 10,    dropout_rate            = 0.1,    output_size             = 3    #quantiles               = 0.5 )  X_static_cat <- array(sample(5, 32 * 2, replace=TRUE), c(32, 2)) - 1 X_static_num <- array(runif(32 * 1), c(32, 1))  X_past_num <- array(runif(32 * 28 * 2), c(32, 28, 2)) X_past_cat <- array(sample(4, 32 * 28 * 2, replace=TRUE), c(32, 28, 5))  X_fut_num <- array(runif(32 * 14 * 5), c(32, 28, 1)) X_fut_cat <- array(sample(4, 32 * 14 * 2, replace=TRUE), c(32, 28, 5))  tft(X_past_num, X_past_cat, X_fut_num, X_fut_cat, X_static_num, X_static_cat) #> Error in py_call_impl(callable, dots$args, dots$keywords): tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."},{"path":"/reference/safe_var.html","id":null,"dir":"Reference","previous_headings":"","what":"Var function, which accepts one-element vectors — safe_var","title":"Var function, which accepts one-element vectors — safe_var","text":"Var function, accepts one-element vectors","code":""},{"path":"/reference/safe_var.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Var function, which accepts one-element vectors — safe_var","text":"","code":"safe_var(x)"},{"path":"/reference/ts_generator.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a generator for time series data — ts_generator","title":"Create a generator for time series data — ts_generator","text":"advantage generator explicit arrays creation make_arrays beginning lower RAM space volume needed kind operation. full arrays created, allocate space examples selected passed data.frame. use ts_generator instead, following examples deliver batch sub-arrays created fly. means store examples RAM time.","code":""},{"path":"/reference/ts_generator.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a generator for time series data — ts_generator","text":"","code":"ts_generator(   data,   key,   index,   lookback,   horizon,   stride = 1,   shuffle = TRUE,   sample_frac = 1,   target,   numeric = NULL,   categorical = NULL,   static = NULL,   past = NULL,   future = NULL,   batch_size = 1,   ... )"},{"path":"/reference/ts_generator.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a generator for time series data — ts_generator","text":"lookback length context past horizon forecast length stride Stride moving window shuffle Shuffle samples. Set FALSE test dataset. target Target variable(s) numeric Numeric variables categorical Categorical variables static Static variables batch_size Batch size","code":""},{"path":"/reference/ts_generator.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a generator for time series data — ts_generator","text":"","code":"library(m5) library(recipes, warn.conflicts=FALSE) library(zeallot) library(dplyr, warn.conflicts=FALSE) library(data.table, warn.conflicts=FALSE)  # ========================================================================== #                          PREPARING THE DATA # ==========================================================================  train <- tiny_m5[date < '2016-01-01'] test  <- tiny_m5[date >= '2016-01-01']  m5_recipe <-    recipe(value ~ ., data=train) %>%    step_mutate(item_id_idx=item_id, store_id_idx=store_id) %>%    step_integer(item_id_idx, store_id_idx,                 wday, month,                 event_name_1, event_type_1,                 event_name_2, event_type_2,                 zero_based=TRUE) %>%    step_naomit(all_predictors()) %>%    prep()  train <- bake(m5_recipe, train) test  <- bake(m5_recipe, test)  TARGET      <- 'value' STATIC      <- c('item_id_idx', 'store_id_idx') CATEGORICAL <- c('event_name_1', 'event_type_1', STATIC) NUMERIC     <- c('sell_price', 'sell_price') KEY         <- c('item_id', 'store_id') INDEX       <- 'date' LOOKBACK    <- 28 HORIZON     <- 14 STRIDE      <- LOOKBACK BATCH_SIZE  <- 32  setDT(train) setDT(test)  # ========================================================================== #                          CREATING GENERATORS # ==========================================================================  c(train_generator, train_steps) %<-%    ts_generator(        data = train,        key = KEY,        index = INDEX,        lookback = LOOKBACK,        horizon = HORIZON,        stride = STRIDE,        target=TARGET,        static=STATIC,        categorical=CATEGORICAL,        numeric=NUMERIC,        batch_size=BATCH_SIZE    )  batch <- train_generator() print(names(batch)) #> [1] \"y_past\"       \"X_past_num\"   \"X_past_cat\"   \"y_fut\"        \"X_fut_num\"    #> [6] \"X_fut_cat\"    \"X_static_cat\"  test_generator <-    ts_generator(        data = test,        key = KEY,        index = INDEX,        lookback = LOOKBACK,        horizon = HORIZON,        stride = STRIDE,        target=TARGET,        static=STATIC,        categorical=CATEGORICAL,        numeric=NUMERIC    )"}]
