<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="description" content="A layer of trainable low-dimensional delay systems.
Each unit buffers its encoded input
by internally representing a low-dimensional
(i.e., compressed) version of the sliding window.
Nonlinear decodings of this representation,
expressed by the A and B matrices, provide
computations across the window, such as its
derivative, energy, median value, etc (1, 2).
Note that these decoder matrices can span across
all of the units of an input sequence."><title>Legendre Memory Unit layer — layer_lmu • aion</title><!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png"><link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png"><link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png"><link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png"><link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png"><link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png"><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.1.0/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.1.0/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous"><!-- bootstrap-toc --><script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Legendre Memory Unit layer — layer_lmu"><meta property="og:description" content="A layer of trainable low-dimensional delay systems.
Each unit buffers its encoded input
by internally representing a low-dimensional
(i.e., compressed) version of the sliding window.
Nonlinear decodings of this representation,
expressed by the A and B matrices, provide
computations across the window, such as its
derivative, energy, median value, etc (1, 2).
Note that these decoder matrices can span across
all of the units of an input sequence."><!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--><script defer data-domain="pkgdown.r-lib.org,all.tidyverse.org" src="https://plausible.io/js/plausible.js"></script></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">aion</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.0</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="active nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/m5_forecasting.html">M5 Forecasting challenges</a>
    <a class="dropdown-item" href="../articles/preparing_input.html">Preparing input</a>
  </div>
</li>
      </ul><form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off"></form>

      <ul class="navbar-nav"></ul></div>

    
  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Legendre Memory Unit layer</h1>
      
      <div class="d-none name"><code>layer_lmu.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p>A layer of trainable low-dimensional delay systems.
Each unit buffers its encoded input
by internally representing a low-dimensional
(i.e., compressed) version of the sliding window.
Nonlinear decodings of this representation,
expressed by the A and B matrices, provide
computations across the window, such as its
derivative, energy, median value, etc (1<em>, 2</em>).
Note that these decoder matrices can span across
all of the units of an input sequence.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-usage">Usage<a class="anchor" aria-label="anchor" href="#ref-usage"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="fu">layer_lmu</span><span class="op">(</span>
  <span class="va">object</span>,
  <span class="va">memory_d</span>,
  <span class="va">order</span>,
  <span class="va">theta</span>,
  <span class="va">hidden_cell</span>,
  trainable_theta <span class="op">=</span> <span class="cn">FALSE</span>,
  hidden_to_memory <span class="op">=</span> <span class="cn">FALSE</span>,
  memory_to_memory <span class="op">=</span> <span class="cn">FALSE</span>,
  input_to_hidden <span class="op">=</span> <span class="cn">FALSE</span>,
  discretizer <span class="op">=</span> <span class="st">"zoh"</span>,
  kernel_initializer <span class="op">=</span> <span class="st">"glorot_uniform"</span>,
  recurrent_initializer <span class="op">=</span> <span class="st">"orthogonal"</span>,
  kernel_regularizer <span class="op">=</span> <span class="cn">NULL</span>,
  recurrent_regularizer <span class="op">=</span> <span class="cn">NULL</span>,
  use_bias <span class="op">=</span> <span class="cn">FALSE</span>,
  bias_initializer <span class="op">=</span> <span class="st">"zeros"</span>,
  bias_regularizer <span class="op">=</span> <span class="cn">NULL</span>,
  dropout <span class="op">=</span> <span class="fl">0</span>,
  recurrent_dropout <span class="op">=</span> <span class="fl">0</span>,
  return_sequences <span class="op">=</span> <span class="cn">FALSE</span>,
  <span class="va">...</span>
<span class="op">)</span></code></pre></div>
    </div>

    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>
    <dl><dt>memory_d</dt>
<dd><p>Dimensionality of input to memory component.</p></dd>


<dt>order</dt>
<dd><p>The number of degrees in the transfer function of the LTI system used to
represent the sliding window of history. This parameter sets the number of
Legendre polynomials used to orthogonally represent the sliding window.</p></dd>


<dt>theta</dt>
<dd><p>The number of timesteps in the sliding window that is represented using the
LTI system. In this context, the sliding window represents a dynamic range of
data, of fixed size, that will be used to predict the value at the next time
step. If this value is smaller than the size of the input sequence, only that
number of steps will be represented at the time of prediction, however the
entire sequence will still be processed in order for information to be
projected to and from the hidden layer. If <code>trainable_theta</code> is enabled, then
theta will be updated during the course of training.</p></dd>


<dt>hidden_cell</dt>
<dd><p>Keras Layer/RNNCell implementing the hidden component.</p></dd>


<dt>trainable_theta</dt>
<dd><p>If TRUE, theta is learnt over the course of training. Otherwise, it is kept
constant.</p></dd>


<dt>hidden_to_memory</dt>
<dd><p>If TRUE, connect the output of the hidden component back to the memory
component (default FALSE).</p></dd>


<dt>memory_to_memory</dt>
<dd><p>If TRUE, add a learnable recurrent connection (in addition to the static</p></dd>


<dt>input_to_hidden</dt>
<dd><p>If TRUE, connect the input directly to the hidden component (in addition to</p></dd>


<dt>discretizer</dt>
<dd><p>The method used to discretize the A and B matrices of the LMU. Current
options are "zoh" (short for Zero Order Hold) and "euler".
"zoh" is more accurate, but training will be slower than "euler" if
<code>trainable_theta=TRUE</code>. Note that a larger theta is needed when discretizing
using "euler" (a value that is larger than <code>4*order</code> is recommended).</p></dd>


<dt>kernel_initializer</dt>
<dd><p>Initializer for weights from input to memory/hidden component. If <code>NULL</code>,
no weights will be used, and the input size must match the memory/hidden size.</p></dd>


<dt>recurrent_initializer</dt>
<dd><p>Initializer for <code>memory_to_memory</code> weights (if that connection is enabled).</p></dd>


<dt>kernel_regularizer</dt>
<dd><p>Regularizer for weights from input to memory/hidden component.</p></dd>


<dt>recurrent_regularizer</dt>
<dd><p>Regularizer for <code>memory_to_memory</code> weights (if that connection is enabled).</p></dd>


<dt>use_bias</dt>
<dd><p>If TRUE, the memory component includes a bias term.</p></dd>


<dt>bias_initializer</dt>
<dd><p>Initializer for the memory component bias term. Only used if <code>use_bias=TRUE</code>.</p></dd>


<dt>bias_regularizer</dt>
<dd><p>Regularizer for the memory component bias term. Only used if <code>use_bias=TRUE</code>.</p></dd>


<dt>dropout</dt>
<dd><p>Dropout rate on input connections.</p></dd>


<dt>recurrent_dropout</dt>
<dd><p>Dropout rate on <code>memory_to_memory</code> connection.</p></dd>


<dt>return_sequences</dt>
<dd><p>If TRUE, return the full output sequence. Otherwise, return just the last
output in the output sequence.</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a></h2>
    
<ol><li><p>Voelker and Eliasmith (2018). Improving spiking dynamical
networks: Accurate delays, higher-order synapses, and time cells.
Neural Computation, 30(3): 569-609.
2.Voelker and Eliasmith. "Methods and systems for implementing
dynamic neural networks." U.S. Patent Application No. 15/243,223.</p></li>
</ol></div>

  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p></p><p>Developed by <a href="https://krzjoa.github.io" class="external-link">Krzysztof Joachimiak</a>.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p><p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.6.</p>
</div>

    </footer></div>

  

  

  </body></html>

