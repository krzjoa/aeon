% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/layer-interpretable-mh-attention.R
\name{layer_interpretable_mh_attention}
\alias{layer_interpretable_mh_attention}
\title{Interpretable multi-head attention layer}
\usage{
layer_interpretable_mh_attention(
  object,
  state_size,
  num_heads,
  dropout_rate,
  name = NULL,
  ...
)
}
\arguments{
\item{num_heads}{Number of attention heads.}

\item{dropout_rate}{Dropout rate

\href{https://github.com/google-research/google-research/blob/4808a726f4b126ea38d49cdd152a6bb5d42efdf0/tft/libs/tft_model.py#L278}{TFT}}
}
\description{
Interpretable multi-head attention layer
}
