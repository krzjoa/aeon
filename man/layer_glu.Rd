% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/layer-glu.R
\name{layer_glu}
\alias{layer_glu}
\title{Gated Linear Unit}
\usage{
layer_glu(object, units, activation = NULL, return_gate = FALSE, ...)
}
\arguments{
\item{object}{What to compose the new \code{Layer} instance with. Typically a
Sequential model or a Tensor (e.g., as returned by \code{layer_input()}).
The return value depends on \code{object}. If \code{object} is:
\itemize{
\item missing or \code{NULL}, the \code{Layer} instance is returned.
\item a \code{Sequential} model, the model with an additional layer is returned.
\item a Tensor, the output tensor from \code{layer_instance(object)} is returned.
}}

\item{units}{Positive integer, dimensionality of the output space.}

\item{activation}{Name of activation function to use. If you don't specify
anything, no activation is applied (ie. "linear" activation: a(x) = x).}

\item{return_gate}{Logical - return gate values. Default: FALSE}
}
\value{
Tensor of shape (batch_size, ..., units). Optionally, it can also return a weights tensor
with identical shape.
}
\description{
In such form introduced in \href{https://arxiv.org/abs/1612.08083}{Language modeling with gated convolutional networks}
by Dauphin et al., when it was used in sequence processing tasks and compared with
gating mechanism used in LSTM layers. In the context of time series processing explicitly proposed in \href{https://arxiv.org/pdf/1912.09363.pdf}{Temporal Fusion Transformer}.
}
\details{
Computed according to the equation:
\deqn{GLU(\gamma) = \sigma(W\gamma + b) \odot (V\gamma + c)}
}
\section{Input and Output Shapes}{



Input shape: nD tensor with shape: \verb{(batch_size, ..., input_dim)}. The most
common situation would be a 2D input with shape \verb{(batch_size, input_dim)}.

Output shape: nD tensor with shape: \verb{(batch_size, ..., units)}. For
instance, for a 2D input with shape \verb{(batch_size, input_dim)}, the output
would have shape \verb{(batch_size, unit)}.

}

\examples{
library(keras)

# ================================================================
#             SEQUENTIAL MODEL, NO GATE VALUES RETURNED
# ================================================================

model <-
  keras_model_sequential() \%>\%
  layer_glu(10, input_shape = 30)

model

output <- model(matrix(1, 32, 30))
dim(output)
output[1,]

# ================================================================
#                     WITH GATE VALUES RETURNED
# ================================================================

inp  <- layer_input(30)
out  <- layer_glu(units = 10, return_gate = TRUE)(inp)

model <- keras_model(inp, out)

model

c(values, gate) \%<-\% model(matrix(1, 32, 30))
dim(values)
dim(gate)

values[1,]
gate[1,]
}
\references{
\enumerate{
\item Dauphin, Yann N., et al. (2017). \href{https://arxiv.org/abs/1612.08083}{Language modeling with gated convolutional networks.}.
International conference on machine learning. PMLR
\item Lim, Bryan et al. (2019). \href{https://arxiv.org/abs/1612.08083}{Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting}. arXiv
\item \href{https://github.com/jdb78/pytorch-forecasting/blob/268121aa9aa732558beefb6d9f95feff955ad08b/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.py#L71}{Implementation PyTorch by Jan Beitner}
\item \href{https://github.com/PlaytikaResearch/tft-torch/blob/9eee6db42b8ec6b6a586e8852e3af6e2d6b18035/tft_torch/tft.py#L11}{Implementation PyTorch by Playtika Research}
}
}
