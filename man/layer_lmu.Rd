% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/layer-lmu.R
\name{layer_lmu}
\alias{layer_lmu}
\title{Legendre Memory Unit layer}
\usage{
layer_lmu(
  object,
  memory_d,
  order,
  theta,
  hidden_cell,
  trainable_theta = FALSE,
  hidden_to_memory = FALSE,
  memory_to_memory = FALSE,
  input_to_hidden = FALSE,
  discretizer = "zoh",
  kernel_initializer = "glorot_uniform",
  recurrent_initializer = "orthogonal",
  kernel_regularizer = NULL,
  recurrent_regularizer = NULL,
  use_bias = FALSE,
  bias_initializer = "zeros",
  bias_regularizer = NULL,
  dropout = 0,
  recurrent_dropout = 0,
  return_sequences = FALSE,
  ...
)
}
\arguments{
\item{memory_d}{Dimensionality of input to memory component.}

\item{order}{The number of degrees in the transfer function of the LTI system used to
represent the sliding window of history. This parameter sets the number of
Legendre polynomials used to orthogonally represent the sliding window.}

\item{theta}{The number of timesteps in the sliding window that is represented using the
LTI system. In this context, the sliding window represents a dynamic range of
data, of fixed size, that will be used to predict the value at the next time
step. If this value is smaller than the size of the input sequence, only that
number of steps will be represented at the time of prediction, however the
entire sequence will still be processed in order for information to be
projected to and from the hidden layer. If \code{trainable_theta} is enabled, then
theta will be updated during the course of training.}

\item{hidden_cell}{Keras Layer/RNNCell implementing the hidden component.}

\item{trainable_theta}{If TRUE, theta is learnt over the course of training. Otherwise, it is kept
constant.}

\item{hidden_to_memory}{If TRUE, connect the output of the hidden component back to the memory
component (default FALSE).}

\item{memory_to_memory}{If TRUE, add a learnable recurrent connection (in addition to the static}

\item{input_to_hidden}{If TRUE, connect the input directly to the hidden component (in addition to}

\item{discretizer}{The method used to discretize the A and B matrices of the LMU. Current
options are "zoh" (short for Zero Order Hold) and "euler".
"zoh" is more accurate, but training will be slower than "euler" if
\code{trainable_theta=TRUE}. Note that a larger theta is needed when discretizing
using "euler" (a value that is larger than \code{4*order} is recommended).}

\item{kernel_initializer}{Initializer for weights from input to memory/hidden component. If \code{NULL},
no weights will be used, and the input size must match the memory/hidden size.}

\item{recurrent_initializer}{Initializer for \code{memory_to_memory} weights (if that connection is enabled).}

\item{kernel_regularizer}{Regularizer for weights from input to memory/hidden component.}

\item{recurrent_regularizer}{Regularizer for \code{memory_to_memory} weights (if that connection is enabled).}

\item{use_bias}{If TRUE, the memory component includes a bias term.}

\item{bias_initializer}{Initializer for the memory component bias term. Only used if \code{use_bias=TRUE}.}

\item{bias_regularizer}{Regularizer for the memory component bias term. Only used if \code{use_bias=TRUE}.}

\item{dropout}{Dropout rate on input connections.}

\item{recurrent_dropout}{Dropout rate on \code{memory_to_memory} connection.}

\item{return_sequences}{If TRUE, return the full output sequence. Otherwise, return just the last
output in the output sequence.}
}
\description{
A layer of trainable low-dimensional delay systems.
Each unit buffers its encoded input
by internally representing a low-dimensional
(i.e., compressed) version of the sliding window.
Nonlinear decodings of this representation,
expressed by the A and B matrices, provide
computations across the window, such as its
derivative, energy, median value, etc (\link{1}\emph{, \link{2}}).
Note that these decoder matrices can span across
all of the units of an input sequence.
}
\section{Output shape}{


\itemize{
\item if \code{return_state}: a list of tensors. The first tensor is
the output. The remaining tensors are the last states,
each with shape \verb{(batch_size, state_size)}, where \code{state_size}
could be a high dimension tensor shape.
\item if \code{return_sequences}: N-D tensor with shape \verb{[batch_size, timesteps, output_size]}, where \code{output_size} could be a high dimension tensor shape, or
\verb{[timesteps, batch_size, output_size]} when \code{time_major} is \code{TRUE}
\item else, N-D tensor with shape \verb{[batch_size, output_size]}, where
\code{output_size} could be a high dimension tensor shape.
}

}

\examples{
\dontrun{
library(keras)
inp <- layer_input(c(28, 3))
hidden_cell <- layer_lstm_cell(10)
lmu <- layer_lmu(memory_d=10, order=3, theta=28, hidden_cell=hidden_cell)(inp)
model <- keras_model(inp, lmu)
model(array(1, c(32, 28, 3)))
}
}
\references{
\enumerate{
\item A. Voelker, I. KajiÄ‡ and C. Eliasmith, \href{http://compneuro.uwaterloo.ca/files/publications/voelker.2019.lmu.pdf}{Legendre Memory Units: Continuous-Time Representation in Recurrent Neural Networks} (2019)
\item A. Voelker and C. Eliasmith \href{http://compneuro.uwaterloo.ca/files/publications/voelker.2018.pdf}{Improving spiking dynamical networks: Accurate delays, higher-order synapses, and time cells. Neural Computation, 30(3): 569-609.} (2018)
\item A. Voelker and C. Eliasmith \href{https://patents.google.com/patent/US20180053090A1/en}{Methods and systems for implementing dynamic neural networks. U.S. Patent Application No. 15/243,223.}
\item \href{https://stats.stackexchange.com/questions/472822/is-lstm-long-short-term-memory-dead}{Is LSTM (Long Short-Term Memory) dead?, CrossValidated}
}
}
