% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/layer-scaled-dot-attention.R
\name{layer_scaled_dot_attention}
\alias{layer_scaled_dot_attention}
\title{Scaled dot product attention layer}
\usage{
layer_scaled_dot_attention(object, dropout_rate = 0, ...)
}
\arguments{
\item{dropout_rate}{Dropout rate}
}
\description{
Introduced in \href{https://arxiv.org/pdf/1706.03762v5.pdf}{Attention Is All You Need}.
Defined as:
}
\details{
\deqn{Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V}

Originally, \code{dropout} hasn't been specified there. It was added inside the layer
in the \href{https://github.com/google-research/google-research/blob/4808a726f4b126ea38d49cdd152a6bb5d42efdf0/tft/libs/tft_model.py#L240}{Temporal Fusion Transformer} implementation by Google.
}
